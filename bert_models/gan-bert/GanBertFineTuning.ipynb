{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3f189ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../utils')\n",
    "\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "from ganbert_utils import *\n",
    "from ganbert_models import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b09b77aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c9b10f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Set random values\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28cc2fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla V100-SXM2-16GB\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = get_gpu_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b50c9ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "#  Read Fine Tuning Parameters from FineTuning Configuration File \n",
    "#--------------------------------\n",
    "\n",
    "from ganbert_finetuning_config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "af7e93e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------\n",
    "#  Load the Transformers Models \n",
    "#--------------------------------\n",
    "transformer = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7b60f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "#  Extract the Dataset\n",
    "#--------------------------------\n",
    "\n",
    "labeled_examples, unlabeled_examples, _ = get_sst_examples('./../../data/SST-2/train.tsv',test=False,discard_values = 0.99)\n",
    "_, _, test_examples = get_sst_examples('./../../data/SST-2/dev.tsv', test=True,discard_values = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "95789016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples len is:  681\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------\n",
    "#  Prepare the Training Dataset\n",
    "#--------------------------------\n",
    "label_map = {}\n",
    "\n",
    "for (i, label) in enumerate(label_list):\n",
    "    label_map[label] = i\n",
    "\n",
    "train_examples = labeled_examples\n",
    "#The labeled (train) dataset is assigned with a mask set to True\n",
    "\n",
    "train_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
    "\n",
    "#If unlabel examples are available\n",
    "if unlabeled_examples:\n",
    "    train_examples = train_examples + unlabeled_examples\n",
    "    #The unlabeled (train) dataset is assigned with a mask set to False \n",
    "    tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
    "    train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
    "    \n",
    "    \n",
    "train_dataloader = generate_data_loader(train_examples, \n",
    "                                        train_label_masks, \n",
    "                                        label_map, \n",
    "                                        tokenizer, \n",
    "                                        batch_size=64, \n",
    "                                        do_shuffle = True\n",
    "                                       )  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c05e7afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples len is:  1\n"
     ]
    }
   ],
   "source": [
    "#------------------------------\n",
    "#   Prepare the Test Dataset\n",
    "#------------------------------\n",
    "#The labeled (test) dataset is assigned with a mask set to True\n",
    "test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
    "\n",
    "test_dataloader = generate_data_loader(test_examples, \n",
    "                                       test_label_masks, \n",
    "                                       label_map, \n",
    "                                       tokenizer, \n",
    "                                       batch_size=64,\n",
    "                                       do_shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "738a00b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_sizes[i] and [i+1] is 768 and 768\n"
     ]
    }
   ],
   "source": [
    "#------------------------------\n",
    "#   Prepare the Generator and Discriminator Outputs \n",
    "#------------------------------\n",
    "\n",
    "# The config file is required to get the dimension of the vector produced by \n",
    "# the underlying transformer\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "# currently this notebook has the hidden size of the encoder from bert with 768\n",
    "hidden_size = int(config.hidden_size)\n",
    "\n",
    "\n",
    "# Define the number and width of hidden layers\n",
    "hidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\n",
    "hidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n",
    "\n",
    "\n",
    "#-------------------------------------------------\n",
    "#   Instantiate the Generator and Discriminator\n",
    "#-------------------------------------------------\n",
    "generator = Generator(noise_size=noise_size, \n",
    "                      output_size=hidden_size, \n",
    "                      hidden_sizes=hidden_levels_g, \n",
    "                      dropout_rate=out_dropout_rate\n",
    "                     )\n",
    "\n",
    "discriminator = Discriminator(input_size=hidden_size, \n",
    "                              hidden_sizes=hidden_levels_d, \n",
    "                              num_labels=len(label_list), \n",
    "                              dropout_rate=out_dropout_rate\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3778be09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[768]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_levels_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c3ca518d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "#   Transfer Objects to GPU\n",
    "#-------------------------------------------------\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    transformer.cuda()\n",
    "    \n",
    "    if multi_gpu:\n",
    "        transformer = torch.nn.DataParallel(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ec222ad7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "  Batch    10  of     11.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss generetor: 0.457\n",
      "  Average training loss discriminator: 2.160\n",
      "  Training epcoh took: 0:00:04\n",
      "\n",
      "Running Test...\n",
      "Evaluation tensor([[-7.4150e-01,  6.8245e-01,  9.9999e-01, -9.9925e-01,  9.3446e-01,\n",
      "          6.9396e-01,  9.9420e-01, -9.7315e-01, -9.7911e-01,  2.1464e-01,\n",
      "          9.9354e-01,  9.9985e-01, -9.7603e-01, -9.9991e-01,  6.1979e-01,\n",
      "         -9.9032e-01,  9.9743e-01, -8.9881e-01, -1.0000e+00, -2.1543e-01,\n",
      "         -4.6660e-01, -9.9999e-01,  2.6799e-01,  9.3068e-01,  9.5611e-01,\n",
      "          4.8368e-01,  9.9810e-01,  1.0000e+00,  8.8602e-01,  5.5357e-01,\n",
      "          5.9344e-01, -9.9866e-01,  8.7762e-01, -9.9997e-01,  5.4566e-01,\n",
      "         -1.6473e-01,  8.0411e-01, -6.8427e-01,  9.0533e-01, -8.4689e-01,\n",
      "         -5.9253e-01, -6.7736e-01,  4.5640e-02, -7.3658e-01,  7.0170e-01,\n",
      "          4.8934e-01,  4.7939e-01,  3.4084e-01, -1.2402e-01,  9.9998e-01,\n",
      "         -9.7787e-01,  1.0000e+00, -9.7183e-01,  9.9987e-01,  9.9849e-01,\n",
      "          7.1873e-01,  9.9918e-01,  3.7117e-01, -9.9397e-01,  7.3181e-01,\n",
      "          9.6579e-01, -6.0400e-01,  9.5288e-01, -1.3237e-01, -4.2771e-01,\n",
      "         -7.2167e-01, -5.7099e-01,  5.5742e-01, -3.8540e-01,  7.8075e-01,\n",
      "          1.9085e-01,  4.9305e-01,  9.9686e-01, -9.9005e-01, -3.7737e-01,\n",
      "         -9.7670e-01,  5.5759e-01, -9.9999e-01,  9.8826e-01,  9.9999e-01,\n",
      "          4.4909e-01, -9.9997e-01,  9.9952e-01, -5.2608e-01, -6.3314e-01,\n",
      "         -4.2533e-01, -9.8237e-01, -9.9997e-01,  7.1553e-01, -5.6626e-01,\n",
      "          8.9325e-01, -9.9842e-01,  8.4534e-01, -9.3756e-01,  1.0000e+00,\n",
      "         -5.9941e-01, -5.4889e-01,  6.6580e-01,  7.1548e-01, -5.7644e-01,\n",
      "         -8.5039e-01,  7.6998e-01,  9.8660e-01, -9.3899e-01,  9.9110e-01,\n",
      "         -1.1302e-01, -9.7276e-01, -5.9511e-01, -8.5188e-01,  5.6452e-01,\n",
      "          9.9310e-01, -9.9184e-01, -6.1284e-01,  6.3305e-01,  7.7528e-01,\n",
      "         -9.8909e-01,  9.9753e-01,  8.0959e-01, -8.3333e-01,  1.0000e+00,\n",
      "          9.0313e-02,  8.5362e-01,  9.9984e-01,  4.3133e-01, -6.6651e-01,\n",
      "         -6.0001e-01, -5.3688e-01,  7.4894e-01,  3.9528e-01, -5.6624e-01,\n",
      "          9.0743e-01, -9.9864e-01, -9.8027e-01,  9.9997e-01, -5.5650e-01,\n",
      "          1.0000e+00, -9.9987e-01,  9.5391e-01, -9.9999e-01, -8.6263e-01,\n",
      "         -8.6231e-01,  2.7478e-01, -9.4655e-01,  6.9263e-01,  9.9780e-01,\n",
      "          3.2541e-01, -8.0440e-01, -8.1185e-01,  1.6293e-01, -8.0346e-01,\n",
      "          9.2116e-01,  9.8192e-01, -9.7838e-01,  9.9992e-01,  9.7873e-01,\n",
      "          9.2228e-01,  8.4184e-01,  6.4702e-01, -7.6655e-01,  9.0788e-01,\n",
      "          9.9790e-01, -9.9994e-01, -8.9923e-01, -9.8332e-01,  9.9994e-01,\n",
      "          9.9295e-01,  6.0125e-01, -9.7042e-01,  9.9998e-01,  6.5378e-01,\n",
      "         -5.8275e-01, -7.7757e-01, -5.0792e-01, -9.8475e-01,  7.0111e-01,\n",
      "          8.0405e-01,  8.3065e-01,  9.9998e-01, -9.9844e-01,  9.9980e-01,\n",
      "          9.9806e-01,  8.2528e-01,  9.1412e-01,  9.8972e-01, -9.9964e-01,\n",
      "         -9.9764e-01, -9.9879e-01,  5.7680e-01,  8.1633e-01, -2.8682e-01,\n",
      "          7.2635e-01,  9.8620e-01,  9.8046e-01,  6.2881e-01, -9.9979e-01,\n",
      "         -3.8157e-01,  9.9767e-01, -6.1136e-01,  1.0000e+00,  9.1038e-01,\n",
      "         -9.9999e-01, -8.3789e-01,  9.0669e-01,  9.1705e-01, -5.0204e-01,\n",
      "          9.8771e-01, -7.7320e-01, -4.4497e-03,  9.9129e-01, -9.9996e-01,\n",
      "          9.5115e-01,  5.4604e-01,  7.0163e-01,  7.7143e-01,  9.9786e-01,\n",
      "         -6.0256e-01, -3.4311e-01,  6.4486e-01, -2.5192e-01,  9.9999e-01,\n",
      "         -9.9996e-01,  2.2899e-02,  6.2317e-01, -9.9965e-01, -9.9961e-01,\n",
      "          9.9656e-01, -2.9272e-01,  6.7558e-01, -4.9319e-01, -2.7396e-01,\n",
      "          4.5821e-01,  6.8635e-01,  9.9893e-01, -1.3593e-01,  3.8222e-01,\n",
      "         -9.9999e-01, -9.6846e-01, -8.2770e-01, -9.7096e-01,  3.3668e-01,\n",
      "          7.2038e-01, -6.3831e-01, -9.4464e-01, -9.7907e-01,  9.8084e-01,\n",
      "          9.3368e-01, -9.1951e-01, -1.9346e-01, -1.4757e-01, -9.8627e-01,\n",
      "         -3.5831e-01, -4.2881e-01, -9.9990e-01,  9.9997e-01, -8.7641e-01,\n",
      "          9.7703e-01,  9.9456e-01, -9.9956e-01,  9.1532e-01, -9.8076e-01,\n",
      "          2.6237e-01, -9.9997e-01, -5.1786e-01, -2.5084e-01, -9.2434e-01,\n",
      "         -1.6546e-01,  9.9905e-01, -9.7278e-01, -5.8874e-01,  5.8098e-01,\n",
      "         -9.9999e-01,  8.8295e-01, -5.0701e-01,  9.9995e-01,  6.3482e-02,\n",
      "          9.0993e-01,  9.9546e-01,  9.1210e-01, -9.9849e-01, -9.9999e-01,\n",
      "          8.9117e-01,  9.9972e-01, -9.9916e-01, -5.4175e-01,  9.9999e-01,\n",
      "         -9.4352e-01, -8.6590e-01, -9.9417e-01, -9.9900e-01, -9.9998e-01,\n",
      "          3.3085e-01, -7.4184e-01, -5.7998e-01,  9.9013e-01,  1.3623e-01,\n",
      "         -4.1091e-02,  9.9961e-01,  9.9951e-01, -4.0324e-01, -2.3766e-02,\n",
      "          5.9183e-01, -9.8914e-01, -9.9982e-01,  1.0978e-01,  5.6712e-01,\n",
      "         -1.0000e+00,  9.9998e-01, -9.9890e-01,  9.9998e-01,  7.8305e-01,\n",
      "         -9.5978e-01,  8.6851e-01, -2.1906e-01, -6.7358e-01,  4.8850e-01,\n",
      "          9.9999e-01,  9.9368e-01, -4.1037e-01,  4.9176e-01,  8.5108e-01,\n",
      "          2.2711e-01,  3.0237e-01, -7.6808e-01, -2.2407e-01,  3.9988e-01,\n",
      "         -9.4770e-01,  9.8957e-01,  4.9453e-01, -9.9728e-01,  9.6643e-01,\n",
      "         -1.1458e-01,  9.1746e-01, -6.8438e-01,  9.2898e-01,  9.9814e-01,\n",
      "         -9.7973e-02,  1.2813e-02, -4.1857e-01, -9.8054e-01, -9.6490e-01,\n",
      "          3.3798e-01, -9.8729e-01,  1.4268e-01,  2.9954e-02,  9.9630e-01,\n",
      "         -9.9577e-01,  9.9977e-01, -2.9410e-01,  8.7883e-01, -9.7824e-01,\n",
      "          1.0000e+00, -9.9981e-01,  3.7745e-01,  3.5441e-01, -4.9690e-01,\n",
      "         -1.3709e-01,  9.9957e-01,  9.6207e-01,  9.6684e-01,  3.3777e-01,\n",
      "          1.5505e-01,  6.5966e-01,  9.9131e-01, -9.4460e-01,  4.0715e-01,\n",
      "         -9.8298e-01, -1.1209e-01,  9.9927e-01,  9.7715e-01, -7.0581e-01,\n",
      "         -8.8741e-01, -9.8435e-01,  9.5321e-01, -5.3634e-01, -8.5360e-01,\n",
      "         -1.7976e-01, -9.2341e-01,  7.8479e-01,  9.7138e-01, -6.6583e-01,\n",
      "          9.0309e-01,  1.4686e-01, -9.9536e-01,  8.7526e-01,  7.1989e-01,\n",
      "          9.9998e-01, -9.9772e-01, -4.5298e-01,  9.9758e-01, -7.1248e-01,\n",
      "         -8.9080e-01,  7.1008e-01,  9.8834e-01, -9.6799e-01, -5.8692e-01,\n",
      "         -9.9998e-01,  5.9973e-01, -5.8736e-01, -5.7718e-01, -7.6136e-03,\n",
      "          6.0664e-01, -9.4990e-01,  8.5642e-01, -2.1475e-01,  9.3155e-01,\n",
      "          3.0404e-01,  9.7201e-01,  6.0353e-01, -3.9613e-01, -3.9467e-01,\n",
      "          4.8185e-01,  5.1953e-01, -1.2317e-01,  9.8846e-01, -9.8302e-01,\n",
      "          9.9999e-01,  1.4757e-01, -1.0000e+00, -9.7643e-01, -8.6869e-01,\n",
      "         -9.9997e-01, -2.9179e-01, -9.9955e-01,  9.9849e-01,  7.0362e-01,\n",
      "         -9.6839e-01, -9.8256e-01, -9.9977e-01, -9.9996e-01,  8.9030e-01,\n",
      "         -1.1882e-01, -4.3745e-01, -5.0746e-01,  9.5934e-01,  2.3158e-01,\n",
      "          6.2589e-01, -5.3147e-01, -9.8766e-01,  6.9917e-01, -9.6549e-01,\n",
      "         -4.0451e-02, -1.0000e+00, -6.8567e-01,  9.9218e-01, -9.7827e-01,\n",
      "          2.7228e-01, -9.8938e-01, -9.6253e-01, -7.2315e-01,  8.0905e-01,\n",
      "          9.9195e-01, -5.0824e-01,  2.8338e-01, -9.9993e-01,  9.9803e-01,\n",
      "         -7.4663e-01,  3.2611e-01, -9.1039e-01, -9.9643e-01,  9.9998e-01,\n",
      "          4.9169e-01,  3.8070e-02, -4.6700e-01, -9.9996e-01,  9.6015e-01,\n",
      "         -6.5999e-01, -5.7145e-01, -9.9778e-01,  2.5329e-01, -9.6735e-01,\n",
      "         -9.9999e-01,  4.9803e-01,  9.3850e-01,  9.9919e-01,  9.9367e-01,\n",
      "          6.5074e-01, -4.4561e-01, -9.2048e-01,  5.5998e-01, -1.0000e+00,\n",
      "          8.7684e-01,  8.9289e-01, -9.7186e-01, -5.7971e-01,  9.9550e-01,\n",
      "          9.9831e-01, -8.3810e-01, -9.2653e-01,  5.3501e-01, -5.9986e-02,\n",
      "          9.7653e-01, -2.4434e-04, -7.9602e-01,  5.9264e-01, -4.0806e-01,\n",
      "         -9.9841e-01, -9.8521e-01,  9.9975e-01, -9.8448e-01,  9.8325e-01,\n",
      "          9.6947e-01,  9.8802e-01,  4.0171e-01, -5.6246e-01, -9.6237e-01,\n",
      "         -9.9977e-01, -7.8893e-01,  3.3874e-01, -9.9999e-01,  9.9999e-01,\n",
      "         -1.0000e+00,  8.8361e-01,  6.7926e-01,  7.8996e-01,  9.9805e-01,\n",
      "         -6.1661e-02, -1.0000e+00, -9.9999e-01,  5.5970e-01, -1.4503e-01,\n",
      "          9.9869e-01,  7.5821e-02,  5.1562e-01, -3.9284e-01,  8.0798e-01,\n",
      "          9.9979e-01,  8.8347e-01,  9.0389e-01, -9.7591e-01,  9.9997e-01,\n",
      "          1.5113e-01, -9.9995e-01,  9.8346e-01, -9.9996e-01,  9.7419e-01,\n",
      "          9.7465e-01,  9.9558e-01,  9.8925e-01, -9.8775e-01,  1.0000e+00,\n",
      "         -9.9999e-01,  9.9955e-01, -1.0000e+00, -9.8279e-01,  9.9999e-01,\n",
      "         -9.9903e-01, -8.3602e-01, -9.9997e-01, -9.7830e-01,  6.4628e-01,\n",
      "          5.3613e-01, -5.5604e-01,  9.9888e-01, -9.9999e-01, -9.9993e-01,\n",
      "         -7.7401e-01, -7.5167e-01,  1.1915e-02,  9.7947e-01, -4.9128e-01,\n",
      "          9.9947e-01, -8.4642e-02,  9.6369e-01,  2.7087e-01,  9.8620e-01,\n",
      "          9.9995e-01, -7.6871e-01,  8.2391e-01, -9.9888e-01,  9.9518e-01,\n",
      "         -6.3263e-01,  5.3271e-01,  9.7910e-01, -1.6675e-01, -5.6853e-01,\n",
      "          6.7872e-01, -9.9986e-01,  7.4450e-01, -9.6810e-01,  9.5750e-01,\n",
      "          7.5463e-01,  8.4080e-01,  3.9710e-01, -2.9589e-02,  4.4611e-01,\n",
      "         -9.9915e-01,  6.0199e-01, -9.9996e-01,  9.9277e-01, -8.7445e-01,\n",
      "          4.7615e-01, -4.5740e-01,  8.1811e-01, -8.9133e-01,  9.9995e-01,\n",
      "          9.9991e-01, -9.9999e-01,  5.0108e-01,  9.9907e-01, -8.7728e-01,\n",
      "          9.8714e-01, -9.9914e-01, -3.7713e-01,  9.8876e-01, -7.1646e-01,\n",
      "          9.9641e-01, -5.6921e-01, -3.8685e-01,  9.8698e-01, -9.9976e-01,\n",
      "         -9.3779e-01, -7.5766e-01,  3.5419e-01,  7.7509e-01, -9.9594e-01,\n",
      "          4.6196e-02,  8.7113e-01, -5.8290e-01, -9.9998e-01, -1.1247e-01,\n",
      "         -9.9996e-01, -1.5710e-01,  9.9811e-01, -9.2667e-01,  9.9999e-01,\n",
      "         -8.2918e-01, -3.3011e-01, -6.6218e-02, -9.9998e-01, -9.8943e-01,\n",
      "          3.7207e-01, -4.3869e-01, -9.0967e-01,  9.7625e-01,  4.7185e-01,\n",
      "         -5.2607e-02, -9.9999e-01,  5.9279e-01,  9.8834e-01,  7.5543e-01,\n",
      "          9.3549e-01, -6.6976e-01, -9.5316e-01, -8.1688e-01, -7.0890e-01,\n",
      "          5.8318e-01,  8.8920e-01, -9.9139e-01, -5.5834e-01, -9.4293e-01,\n",
      "          1.0000e+00, -9.9983e-01, -9.7843e-01, -9.9540e-01,  7.4633e-02,\n",
      "          8.0334e-01,  7.6764e-01, -7.0210e-02, -8.0797e-01,  9.4929e-01,\n",
      "         -7.1548e-01,  9.9939e-01, -9.9960e-01, -9.9925e-01,  9.9998e-01,\n",
      "         -7.2541e-01, -9.4327e-01,  6.7678e-01, -1.2104e-01, -8.8883e-02,\n",
      "         -8.1680e-03,  9.3976e-01, -8.9540e-01, -5.1538e-01, -9.9980e-01,\n",
      "          7.1259e-02, -3.7695e-01, -9.9511e-01, -9.5170e-01, -6.7142e-01,\n",
      "         -9.9998e-01,  9.9969e-01,  9.8986e-01,  1.0000e+00, -9.9998e-01,\n",
      "          8.8218e-01,  3.6500e-01,  9.9996e-01, -3.2303e-02, -7.0940e-01,\n",
      "          6.8292e-01,  9.9996e-01, -5.7229e-01,  7.9918e-01,  4.2218e-01,\n",
      "         -6.2647e-01,  2.9925e-01, -1.0966e-01,  9.7023e-01, -6.7539e-01,\n",
      "          6.2247e-01, -9.9874e-01, -1.0000e+00,  9.9999e-01, -2.8735e-01,\n",
      "          9.9848e-01,  1.5549e-01,  8.8659e-01, -8.9593e-01,  9.7026e-01,\n",
      "         -9.7616e-01, -9.5915e-01, -1.0000e+00,  4.1500e-01, -9.9999e-01,\n",
      "         -9.9893e-01, -6.2224e-02,  9.9297e-01, -9.9990e-01, -9.8847e-01,\n",
      "          1.4226e-01, -1.0000e+00,  9.5185e-01, -9.6609e-01, -8.9734e-01,\n",
      "         -9.9491e-01,  9.7116e-01, -5.8391e-01, -4.4701e-01,  9.8389e-01,\n",
      "         -9.8735e-01,  8.2636e-01,  7.6557e-01,  9.7118e-01,  4.9072e-01,\n",
      "          1.3811e-01, -6.2292e-01, -9.7704e-01, -7.2459e-01, -9.9168e-01,\n",
      "         -6.7636e-01, -9.9811e-01, -8.4731e-01,  9.9967e-01,  9.9740e-01,\n",
      "         -9.9991e-01, -9.9976e-01,  9.7843e-01, -3.3146e-01,  9.9755e-01,\n",
      "         -6.0789e-01, -9.9999e-01, -1.0000e+00,  3.4274e-01, -2.0789e-01,\n",
      "          9.9908e-01, -2.9583e-01,  9.9990e-01,  8.5365e-01,  6.0770e-01,\n",
      "          6.1850e-01, -3.3357e-01, -3.8786e-01,  2.3098e-01, -6.8684e-01,\n",
      "          1.0000e+00, -4.0580e-01,  9.9860e-01]], device='cuda:0')\n",
      "  Accuracy: 1.000\n",
      "  Test Loss: 0.500\n",
      "  Test took: 0:00:00\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    10  of     11.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss generetor: 0.668\n",
      "  Average training loss discriminator: 1.357\n",
      "  Training epcoh took: 0:00:04\n",
      "\n",
      "Running Test...\n",
      "Evaluation tensor([[-8.5279e-01,  7.7288e-01,  9.9999e-01, -9.9861e-01,  7.7544e-01,\n",
      "          2.2096e-01,  9.7110e-01, -9.3852e-01, -9.3704e-01,  5.6132e-01,\n",
      "          9.9160e-01,  9.9963e-01, -7.5562e-01, -9.9963e-01, -4.6161e-01,\n",
      "         -9.6348e-01,  9.9060e-01, -9.2539e-01, -9.9999e-01,  8.0202e-01,\n",
      "         -2.1448e-01, -9.9998e-01,  1.3397e-01,  7.3605e-01,  8.2761e-01,\n",
      "          5.5782e-01,  9.9611e-01,  9.9999e-01,  7.3785e-01,  9.2882e-01,\n",
      "          6.2071e-01, -9.9556e-01,  1.5654e-01, -9.9996e-01,  5.0783e-01,\n",
      "         -1.2384e-01, -1.4373e-01, -7.5777e-01,  6.1309e-02, -6.7878e-01,\n",
      "         -4.2952e-01,  3.8882e-01, -5.0619e-01, -7.7621e-01,  2.0990e-01,\n",
      "          5.4458e-01,  5.8499e-01,  2.4094e-01, -2.5250e-01,  9.9998e-01,\n",
      "         -9.3519e-01,  1.0000e+00, -8.9717e-01,  9.9973e-01,  9.9552e-01,\n",
      "          6.3183e-01,  9.9803e-01,  4.9103e-01, -9.7208e-01,  8.1051e-01,\n",
      "          9.0079e-01, -7.1699e-01,  8.9276e-01, -2.1737e-01, -8.8667e-01,\n",
      "         -7.9834e-01,  3.7432e-01,  6.1663e-01, -3.7241e-01,  7.5979e-01,\n",
      "         -8.3182e-02,  4.8503e-01,  9.9634e-01, -9.9468e-01, -3.9952e-01,\n",
      "         -9.6911e-01,  9.1355e-01, -9.9997e-01,  9.7312e-01,  9.9998e-01,\n",
      "         -2.5416e-01, -9.9992e-01,  9.9872e-01, -6.6858e-01,  2.7060e-01,\n",
      "         -9.0591e-01, -9.2613e-01, -9.9994e-01,  7.8920e-01, -4.0679e-01,\n",
      "          6.8667e-01, -9.9661e-01,  7.5643e-01, -5.4147e-01,  1.0000e+00,\n",
      "         -1.6645e-01, -6.9579e-01,  6.6287e-01,  3.6741e-01, -3.0729e-01,\n",
      "         -7.9500e-01, -1.0124e-01,  9.4940e-01, -8.2006e-01,  9.7978e-01,\n",
      "         -8.1365e-01, -9.4651e-01, -2.1810e-02, -9.9634e-01,  6.0104e-01,\n",
      "          9.8172e-01, -9.5046e-01,  3.5881e-01,  6.8368e-01,  1.2143e-01,\n",
      "         -9.6570e-01,  9.9445e-01,  1.1015e-01, -8.9323e-01,  9.9999e-01,\n",
      "          3.7751e-01,  6.7437e-01,  9.9962e-01, -6.2171e-01, -1.0879e-01,\n",
      "         -6.7130e-01,  3.2626e-01, -7.5188e-03,  8.8890e-01, -3.6605e-01,\n",
      "          8.8533e-01, -9.9727e-01, -9.3480e-01,  9.9992e-01, -6.2486e-01,\n",
      "          1.0000e+00, -9.9961e-01,  8.1300e-01, -9.9999e-01, -8.5028e-01,\n",
      "         -8.5895e-01,  2.0872e-01, -8.3371e-01,  8.4306e-01,  9.9323e-01,\n",
      "          5.0316e-01, -4.1711e-01, -5.8989e-01, -3.8669e-01, -3.5691e-01,\n",
      "          9.7073e-01,  9.8762e-01, -9.3011e-01,  9.9998e-01,  9.0422e-01,\n",
      "          6.6198e-01,  9.0592e-02,  7.8044e-01, -2.0320e-01,  8.2423e-01,\n",
      "          9.9585e-01, -9.9984e-01, -9.6569e-01, -9.6125e-01,  9.9992e-01,\n",
      "          9.7888e-01, -2.1831e-01, -7.6774e-01,  9.9995e-01,  9.6869e-01,\n",
      "         -6.6681e-01, -9.0617e-01, -8.2324e-01, -9.3066e-01,  6.5982e-01,\n",
      "          8.9358e-01,  7.7207e-01,  9.9998e-01, -9.9601e-01,  9.9973e-01,\n",
      "          9.9988e-01,  9.1185e-01,  8.5358e-01,  9.6930e-01, -9.9944e-01,\n",
      "         -9.9578e-01, -9.9728e-01,  7.0994e-01, -4.4445e-01, -9.0978e-01,\n",
      "          6.3145e-01,  9.6536e-01,  8.3020e-01,  4.1665e-01, -9.9979e-01,\n",
      "         -2.6120e-01,  9.9661e-01, -7.6265e-01,  1.0000e+00,  9.7863e-01,\n",
      "         -9.9999e-01, -3.2226e-01,  4.6396e-01,  6.4685e-01, -5.3163e-01,\n",
      "          9.4455e-01, -6.6845e-01,  1.4686e-01,  9.8365e-01, -9.9999e-01,\n",
      "          8.1570e-01,  5.2173e-01, -1.2497e-01,  5.3238e-01,  9.9203e-01,\n",
      "          3.3613e-01, -4.3466e-01,  6.2442e-01,  3.6031e-01,  9.9998e-01,\n",
      "         -9.9990e-01,  4.2195e-01,  5.5992e-01, -9.9952e-01, -9.9906e-01,\n",
      "          9.9497e-01, -3.3882e-01,  9.3578e-01, -6.3110e-01, -9.2947e-01,\n",
      "          4.2920e-01,  2.0163e-01,  9.9777e-01,  4.8671e-01,  8.8935e-01,\n",
      "         -9.9998e-01, -9.0208e-01, -6.8507e-01, -9.3240e-01,  4.7653e-01,\n",
      "          6.8341e-01, -7.3664e-01, -8.7666e-01, -9.3064e-01,  9.5595e-01,\n",
      "          9.1496e-01, -8.9400e-01, -5.2586e-01,  6.0358e-01, -9.6832e-01,\n",
      "         -9.3832e-01,  5.3680e-01, -9.9985e-01,  9.9994e-01, -5.0789e-01,\n",
      "          9.4827e-01,  9.9247e-01, -9.9882e-01,  9.0130e-01, -9.3237e-01,\n",
      "          1.2270e-01, -9.9998e-01, -8.5068e-01, -9.6824e-01, -8.2074e-01,\n",
      "         -9.8973e-03,  9.9832e-01, -8.8218e-01, -2.2666e-01, -2.9890e-01,\n",
      "         -9.9999e-01,  8.1589e-01, -5.0572e-01,  9.9985e-01, -6.2856e-01,\n",
      "          9.9132e-01,  9.9256e-01,  7.0793e-01, -9.9714e-01, -9.9998e-01,\n",
      "          1.4562e-02,  9.9999e-01, -9.9861e-01, -6.1091e-01,  9.9997e-01,\n",
      "         -6.8646e-01, -7.0323e-01, -9.8852e-01, -9.9904e-01, -9.9993e-01,\n",
      "          3.2267e-01, -2.9036e-01, -7.6274e-01,  9.7387e-01, -6.1767e-01,\n",
      "         -8.8544e-02,  9.9954e-01,  9.9888e-01, -6.0223e-01,  4.5131e-01,\n",
      "          7.2601e-01, -9.8588e-01, -9.9999e-01, -7.9333e-01,  6.1594e-01,\n",
      "         -9.9999e-01,  9.9995e-01, -9.9673e-01,  9.9999e-01,  2.5871e-01,\n",
      "         -7.7888e-01,  8.3746e-01, -3.3870e-01,  1.4769e-01,  6.1044e-01,\n",
      "          9.9999e-01,  9.7297e-01, -5.1295e-01,  5.6931e-01,  8.4189e-01,\n",
      "          3.4674e-01, -7.4547e-01, -5.6591e-01,  5.7331e-01,  3.7395e-01,\n",
      "         -8.7723e-01,  9.4167e-01, -2.1152e-01, -9.9041e-01,  8.1130e-01,\n",
      "         -3.7191e-01,  8.4085e-01,  2.4651e-01,  8.7556e-01,  9.9485e-01,\n",
      "         -2.2682e-01,  7.1380e-01, -6.7840e-01, -9.9938e-01, -9.5253e-01,\n",
      "          3.4867e-01, -8.8199e-01,  7.5119e-01, -9.5998e-01,  9.9108e-01,\n",
      "         -9.8942e-01,  9.9966e-01, -1.9977e-01,  6.8246e-01, -9.4079e-01,\n",
      "          1.0000e+00, -9.9913e-01,  2.6658e-01, -3.3898e-01, -1.2182e-01,\n",
      "          7.1326e-01,  9.9960e-01,  8.5156e-01,  8.9867e-01, -2.0891e-01,\n",
      "          5.4896e-01,  3.1176e-01,  9.8395e-01, -8.2139e-01,  5.0705e-01,\n",
      "         -9.1122e-01,  8.4459e-01,  9.9861e-01,  9.0706e-01, -7.9070e-01,\n",
      "         -9.6825e-01, -9.5376e-01,  8.5055e-01,  5.5698e-01, -5.2782e-01,\n",
      "         -1.8836e-01, -8.6475e-01,  1.8856e-01,  9.0899e-01,  8.8156e-02,\n",
      "          7.6931e-01,  1.1441e-01, -9.8493e-01,  3.9386e-01,  5.2855e-01,\n",
      "          9.9992e-01, -9.9702e-01, -7.9615e-01,  9.9645e-01, -8.1284e-01,\n",
      "         -8.1835e-01,  3.8115e-01,  8.7362e-01, -8.7420e-01, -5.4596e-01,\n",
      "         -9.9993e-01,  6.9242e-01, -5.5566e-01, -6.3088e-01,  8.0769e-01,\n",
      "          6.7810e-01, -9.0890e-01,  5.0386e-01, -2.3492e-01,  9.0407e-01,\n",
      "          3.6197e-01,  9.1057e-01,  8.2413e-01, -5.1325e-01, -4.9271e-01,\n",
      "          7.1885e-01,  6.1051e-01, -3.4320e-01,  9.5964e-01, -9.1669e-01,\n",
      "          9.9999e-01,  3.5110e-01, -9.9999e-01, -9.2777e-01, -3.8703e-01,\n",
      "         -9.9993e-01, -9.7703e-01, -9.9868e-01,  9.9675e-01, -1.9714e-01,\n",
      "         -8.2099e-01, -8.9135e-01, -9.9926e-01, -9.9998e-01,  7.1331e-01,\n",
      "         -8.2396e-01, -5.6344e-01, -7.9935e-01,  9.9840e-01,  2.1956e-01,\n",
      "          7.3293e-01, -4.2672e-01, -9.7442e-01,  9.2613e-01, -8.9419e-01,\n",
      "         -4.8695e-01, -9.9999e-01,  7.9612e-02,  9.5065e-01, -9.3956e-01,\n",
      "          9.3053e-01, -9.8701e-01, -9.7985e-01, -1.4886e-01,  8.7048e-01,\n",
      "          9.7262e-01, -6.4959e-01,  9.4683e-01, -9.9993e-01,  9.9762e-01,\n",
      "          1.2677e-01,  3.1114e-01, -7.7797e-01, -9.9023e-01,  9.9995e-01,\n",
      "         -2.9312e-01,  6.3151e-02, -5.6969e-01, -9.9996e-01,  8.2713e-01,\n",
      "          3.8783e-01,  4.8873e-01, -9.9467e-01,  2.9892e-01, -8.8512e-01,\n",
      "         -9.9999e-01,  7.0148e-01,  6.9118e-01,  9.9745e-01,  9.8983e-01,\n",
      "          6.5424e-01, -4.6154e-01, -7.5160e-01,  7.3318e-01, -9.9999e-01,\n",
      "          6.8882e-01,  7.3804e-01, -9.2047e-01,  8.0270e-01,  9.8261e-01,\n",
      "          9.9799e-01, -6.1764e-02, -7.2396e-01, -4.7509e-01, -3.3422e-02,\n",
      "          8.4024e-01,  9.1305e-01, -7.8376e-01,  7.3897e-01, -4.9546e-01,\n",
      "         -9.9547e-01, -9.7814e-01,  9.9946e-01, -9.5198e-01,  9.5726e-01,\n",
      "          9.3226e-01,  9.7229e-01,  6.3911e-01, -6.4451e-01, -8.9911e-01,\n",
      "         -9.9974e-01, -7.9441e-01, -4.0898e-02, -9.9998e-01,  9.9996e-01,\n",
      "         -9.9999e-01,  6.9227e-01,  9.8646e-01,  2.2450e-01,  9.9566e-01,\n",
      "          4.8788e-02, -9.9999e-01, -9.9999e-01,  1.0218e-01, -4.5124e-01,\n",
      "          9.9669e-01, -1.4070e-01,  3.6714e-01,  1.4586e-01,  9.4104e-01,\n",
      "          9.9955e-01,  9.9520e-01,  9.9830e-01, -9.2846e-01,  9.9993e-01,\n",
      "         -7.6775e-02, -9.9993e-01,  9.2461e-01, -9.9992e-01,  9.5539e-01,\n",
      "          9.1892e-01,  9.9796e-01,  9.6144e-01, -9.4506e-01,  9.9999e-01,\n",
      "         -9.9999e-01,  9.9864e-01, -1.0000e+00, -9.1781e-01,  9.9996e-01,\n",
      "         -9.9789e-01, -6.8906e-01, -9.9990e-01, -8.1991e-01, -2.4358e-03,\n",
      "          7.0310e-01, -6.4985e-01,  9.9797e-01, -9.9998e-01, -9.9989e-01,\n",
      "         -9.5672e-01, -5.8294e-01,  7.6286e-01,  9.3048e-01,  1.2378e-01,\n",
      "          9.9942e-01,  3.3016e-02,  8.6911e-01, -3.7298e-02,  9.0919e-01,\n",
      "          9.9996e-01, -1.7490e-01,  8.7779e-01, -9.9723e-01,  9.8863e-01,\n",
      "          3.0485e-01,  5.1951e-01,  9.5510e-01, -2.2789e-01,  1.0222e-01,\n",
      "          6.6425e-01, -9.9976e-01,  7.8482e-01, -9.9137e-01,  9.4656e-01,\n",
      "          5.2624e-01,  7.3546e-01,  5.7240e-01,  5.0113e-01,  5.9025e-01,\n",
      "         -9.9783e-01,  5.1124e-01, -9.9989e-01,  9.8570e-01, -4.2882e-01,\n",
      "          5.2185e-01, -5.2358e-01,  9.2505e-01, -8.1973e-02,  9.9987e-01,\n",
      "          9.9986e-01, -9.9999e-01,  5.9246e-01,  9.9902e-01, -7.3292e-01,\n",
      "          9.5931e-01, -9.9814e-01, -3.9676e-01,  9.8275e-01, -6.0078e-01,\n",
      "          9.9253e-01, -6.9569e-01, -4.9848e-01,  9.6293e-01, -9.9970e-01,\n",
      "         -6.1490e-01, -5.4657e-01,  2.4904e-01,  7.5732e-01, -9.9341e-01,\n",
      "          3.4044e-02,  1.3252e-01, -6.5876e-01, -9.9997e-01, -6.9351e-01,\n",
      "         -9.9989e-01, -1.2641e-01,  9.9715e-01, -9.4710e-01,  9.9998e-01,\n",
      "         -8.1442e-01, -5.2538e-01, -1.6773e-01, -9.9996e-01, -9.3035e-01,\n",
      "          4.2602e-01, -5.6783e-01, -7.4378e-01,  8.6444e-01,  6.7036e-01,\n",
      "         -9.4771e-01, -9.9999e-01,  5.2519e-01,  9.4818e-01,  7.5240e-01,\n",
      "          9.4588e-01,  1.3728e-01, -7.7766e-01, -6.6374e-02, -7.2277e-01,\n",
      "          7.2300e-01,  6.9405e-01, -9.8230e-01,  9.9905e-02, -9.1787e-01,\n",
      "          9.9999e-01, -9.9964e-01, -9.7605e-01, -9.7274e-01, -4.4598e-01,\n",
      "          9.1124e-02,  7.8983e-01, -3.1279e-02, -4.8279e-01,  9.3176e-01,\n",
      "         -3.4523e-01,  9.9841e-01, -9.9921e-01, -9.9845e-01,  9.9994e-01,\n",
      "         -9.9186e-01, -6.5849e-01,  6.7388e-01,  9.5077e-04, -5.1523e-01,\n",
      "         -2.1191e-02,  9.0562e-01, -5.6441e-01, -7.0997e-01, -9.9944e-01,\n",
      "         -9.0791e-01,  5.7317e-01, -9.9231e-01, -9.5069e-01, -8.4028e-01,\n",
      "         -1.0000e+00,  9.9960e-01,  9.7555e-01,  1.0000e+00, -9.9995e-01,\n",
      "          8.0864e-01,  4.4773e-01,  9.9992e-01,  1.0685e-01, -6.6700e-01,\n",
      "         -1.4427e-01,  9.9992e-01,  7.0759e-01, -3.3107e-01,  3.8966e-01,\n",
      "         -7.0763e-01, -8.4610e-02,  5.1492e-02,  8.6314e-01,  1.1836e-01,\n",
      "          8.7873e-01, -9.9887e-01, -9.9999e-01,  9.9998e-01, -2.6904e-01,\n",
      "          9.9709e-01, -1.5648e-01,  5.1780e-01, -7.3557e-01,  7.2956e-01,\n",
      "         -9.4931e-01, -9.6122e-01, -9.9999e-01,  4.9123e-01, -1.0000e+00,\n",
      "         -9.9783e-01, -2.0623e-02,  9.8736e-01, -9.9964e-01, -9.4921e-01,\n",
      "          3.5215e-01, -1.0000e+00,  8.1285e-01, -9.2143e-01, -5.7267e-01,\n",
      "         -9.8985e-01,  9.2187e-01, -4.5944e-01,  6.0335e-01,  9.3221e-01,\n",
      "         -9.5496e-01, -3.0702e-01,  2.8229e-01,  9.9898e-01,  5.8719e-01,\n",
      "          3.5694e-01,  4.1859e-01, -7.9771e-01, -3.2988e-01, -9.7337e-01,\n",
      "         -9.7405e-01, -9.9628e-01, -4.1625e-01,  9.9918e-01,  9.9584e-01,\n",
      "         -9.9982e-01, -9.9969e-01,  9.2425e-01, -8.0767e-01,  9.9488e-01,\n",
      "         -6.7717e-01, -9.9998e-01, -9.9999e-01,  3.6609e-01, -3.4262e-01,\n",
      "          9.9745e-01, -2.2527e-01,  9.9999e-01,  8.0316e-01,  8.8562e-01,\n",
      "          9.4950e-02,  2.7179e-01, -3.5074e-01,  7.7382e-01, -8.3416e-01,\n",
      "          9.9999e-01,  4.5703e-01,  9.9633e-01]], device='cuda:0')\n",
      "  Accuracy: 1.000\n",
      "  Test Loss: 0.200\n",
      "  Test took: 0:00:00\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    10  of     11.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss generetor: 0.836\n",
      "  Average training loss discriminator: 0.818\n",
      "  Training epcoh took: 0:00:04\n",
      "\n",
      "Running Test...\n",
      "Evaluation tensor([[-0.9492,  0.9070,  1.0000, -0.9986,  0.8859, -0.9606,  0.9923,  0.8147,\n",
      "         -0.9540,  0.0823,  0.9951,  0.9995,  0.9789, -0.9999, -0.9936, -0.9744,\n",
      "          0.9887, -0.9723, -1.0000,  0.9912,  0.6667, -1.0000,  0.3785, -0.7506,\n",
      "          0.9519,  0.5295,  0.9970,  1.0000,  0.8861,  0.9973,  0.7669, -0.9947,\n",
      "         -0.9471, -1.0000,  0.6547, -0.5219, -0.9792, -0.8559, -0.9202,  0.6664,\n",
      "         -0.8546,  0.9735, -0.9693, -0.8377, -0.9750,  0.7970,  0.8623,  0.2750,\n",
      "         -0.6277,  1.0000, -0.9883,  1.0000,  0.8983,  1.0000,  0.9960,  0.7435,\n",
      "          0.9964,  0.7540,  0.8464,  0.9589,  0.8812, -0.8493,  0.9723, -0.5911,\n",
      "         -0.9940, -0.9632,  0.9709,  0.8433, -0.7888,  0.9317,  0.6503,  0.7370,\n",
      "          0.9987, -0.9972, -0.6725, -0.9838,  0.9906, -1.0000,  0.9904,  1.0000,\n",
      "         -0.9847, -0.9999,  0.9979, -0.8877,  0.9849, -0.9998,  0.8722, -1.0000,\n",
      "          0.8282, -0.6241, -0.7071, -0.9975, -0.1972,  0.9360,  1.0000,  0.9668,\n",
      "         -0.8819,  0.7990, -0.9283,  0.9731, -0.9027, -0.9868, -0.8206,  0.9312,\n",
      "         -0.3234, -0.9969, -0.9516,  0.9435, -1.0000,  0.8022,  0.9889, -0.9866,\n",
      "          0.9900,  0.8909, -0.9527, -0.0622,  0.9941, -0.8852, -0.9526,  1.0000,\n",
      "          0.2285, -0.6615,  0.9995, -0.9930,  0.9633, -0.8438,  0.9839, -0.9706,\n",
      "          0.9946, -0.7130,  0.9258, -0.9971,  0.7051,  0.9999, -0.8281,  1.0000,\n",
      "         -0.9997, -0.8761, -1.0000,  0.7544,  0.5043,  0.0797,  0.8244,  0.9691,\n",
      "          0.9932,  0.8361,  0.9429,  0.8908, -0.9450,  0.9367,  0.9941,  0.9950,\n",
      "         -0.9570,  1.0000, -0.8922, -0.8507, -0.9691,  0.8936,  0.9643, -0.3082,\n",
      "          0.9974, -0.9997, -0.9990,  0.3801,  1.0000,  0.9884, -0.9670,  0.9879,\n",
      "          1.0000,  0.9996, -0.2061, -0.9845, -0.9727,  0.9286,  0.6980,  0.9498,\n",
      "          0.9323,  1.0000, -0.9921,  1.0000,  1.0000,  0.7528,  0.8958, -0.8219,\n",
      "         -0.9993, -0.9984, -0.9981,  0.8702, -0.9980, -0.9974, -0.4556,  0.9839,\n",
      "         -0.9691,  0.7915, -1.0000, -0.4972,  0.9964, -0.9094,  1.0000,  0.9978,\n",
      "         -1.0000,  0.7389, -0.9485,  0.6588, -0.7921,  0.9612,  0.6153,  0.7563,\n",
      "          0.1837, -1.0000, -0.9678, -0.3606, -0.9903,  0.6308,  0.9921,  0.9882,\n",
      "         -0.7440,  0.7578,  0.9453,  1.0000, -0.9999,  0.4131,  0.8199, -0.9997,\n",
      "         -0.9987,  0.9975, -0.6084,  0.9987, -0.8429, -1.0000,  0.6160, -0.9624,\n",
      "          0.9974,  0.9776,  0.9994, -1.0000,  0.9223, -0.8808,  0.2538,  0.7834,\n",
      "          0.7720, -0.8977, -0.9237,  0.9109,  0.9790,  0.2052,  0.2090, -0.9633,\n",
      "          0.9948,  0.5906, -0.9984,  0.9898, -1.0000,  1.0000,  0.9359, -0.6026,\n",
      "          0.9985, -0.9991, -0.2491,  0.9033, -0.1260, -1.0000, -0.9839, -0.9998,\n",
      "          0.5014, -0.3533,  0.9980, -0.9198,  0.8768, -0.9885, -1.0000,  0.9463,\n",
      "         -0.6904,  0.9997, -0.9915,  0.9989,  0.9962, -0.8384, -0.9967, -1.0000,\n",
      "         -0.9816,  1.0000, -0.9988, -0.7930,  1.0000,  0.9889, -0.8051, -0.9880,\n",
      "         -0.9995, -0.9999, -0.3652,  0.9597, -0.9001,  0.9845, -0.9852, -0.5833,\n",
      "          0.9996,  1.0000, -0.8813,  0.9180,  0.8945, -0.9873, -1.0000, -0.9954,\n",
      "          0.8490, -1.0000,  1.0000, -0.9923,  1.0000, -0.9683,  0.9832,  0.9472,\n",
      "         -0.6917,  0.9885,  0.7419,  1.0000,  0.9697, -0.7152,  0.7674, -0.3091,\n",
      "          0.8104, -0.9953,  0.8362,  0.9807,  0.5972, -0.9303, -0.6261, -0.9835,\n",
      "         -0.9906, -0.9383,  0.0536,  0.8801,  0.9855,  0.8993,  0.9933, -0.6196,\n",
      "          0.9793, -0.9146, -1.0000,  0.2237,  0.7965,  0.9007,  0.9735, -0.9996,\n",
      "          0.9924, -0.9827,  1.0000, -0.4029, -0.6814,  0.8537,  1.0000, -1.0000,\n",
      "          0.5477, -0.9658,  0.9423,  0.9936,  0.9997, -0.9122, -0.6607,  0.6497,\n",
      "          0.9763,  0.6213,  0.9898,  0.4832,  0.6698,  0.9554,  0.9989,  0.9997,\n",
      "         -0.9362, -0.8747, -0.9951,  0.8002,  0.8857,  0.9890,  0.8048, -0.6407,\n",
      "          0.2757, -0.8709, -0.8952,  0.9644, -0.4326,  0.2975, -0.9925, -0.9169,\n",
      "         -0.7666,  0.9999, -0.9984, -0.9823,  0.9966, -0.9240,  0.2750,  0.4940,\n",
      "         -0.9807, -0.8397, -0.6363, -0.9999,  0.7711, -0.8710, -0.0472,  0.9972,\n",
      "          0.7075,  0.3075, -0.9338, -0.6536,  0.9439,  0.8686,  0.9414,  0.9776,\n",
      "         -0.7166, -0.7775,  0.9628,  0.8612, -0.8498,  0.9779, -0.9688,  1.0000,\n",
      "         -0.6185, -1.0000,  0.8698,  0.9401, -1.0000, -0.9997, -1.0000,  0.9959,\n",
      "         -0.9859,  0.9808,  0.9707, -1.0000, -1.0000, -0.5729, -0.9939, -0.8238,\n",
      "         -0.9790,  1.0000,  0.5513,  0.9399, -0.5714, -0.9835,  0.9971,  0.8888,\n",
      "         -0.9738, -1.0000,  0.9840, -0.8488,  0.8116,  0.9987, -0.9930, -0.6931,\n",
      "         -0.0493,  0.9596,  0.9840,  0.1006,  0.9995, -1.0000,  0.9987,  0.9753,\n",
      "          0.5519,  0.6923, -0.9924,  1.0000, -0.9858,  0.5342, -0.8016, -1.0000,\n",
      "         -0.8911,  0.9893,  0.9881, -0.9969,  0.6173, -0.9512, -1.0000,  0.8999,\n",
      "         -0.9751,  1.0000,  0.9903,  0.9179, -0.6700, -0.8532,  0.8921, -1.0000,\n",
      "         -0.8388, -0.9115, -0.9537,  0.9990,  0.9818,  0.9990,  0.9831,  0.6537,\n",
      "         -0.9958,  0.5459,  0.8259,  0.9995, -0.8903,  0.9097, -0.7241, -0.9954,\n",
      "         -0.9927,  0.9993,  0.8997,  0.9508, -0.4505, -0.6549,  0.9429, -0.8670,\n",
      "          0.3330, -1.0000, -0.9431, -0.7101, -1.0000,  1.0000, -1.0000, -0.3119,\n",
      "          0.9999, -0.9497,  0.9949,  0.6558, -1.0000, -1.0000, -0.7075, -0.9000,\n",
      "          0.9941, -0.7174,  0.4924,  0.9476,  0.9963,  0.9996,  0.9999,  1.0000,\n",
      "          0.9198,  0.9999,  0.4335, -0.9999, -0.6073, -1.0000, -0.0802,  0.9002,\n",
      "          0.9995,  0.9908,  0.9387,  1.0000, -1.0000,  1.0000, -1.0000,  0.9421,\n",
      "          1.0000, -0.9982,  0.8048, -0.9999,  0.9494, -0.9806,  0.8817, -0.8491,\n",
      "          0.9977, -1.0000, -0.9998, -0.9049,  0.8161,  0.9964, -0.7249,  0.9548,\n",
      "          0.9996,  0.4754,  0.8966, -0.7831, -0.9131,  1.0000,  0.8661,  0.9944,\n",
      "         -0.9968,  0.9910,  0.9848,  0.6967,  0.9727, -0.4922,  0.9513,  0.8731,\n",
      "         -0.9997,  0.4405, -1.0000, -0.3206, -0.8459,  0.8788,  0.8419,  0.9389,\n",
      "          0.8040, -0.9970,  0.5864, -0.9999,  0.9899,  0.9204,  0.7774, -0.8106,\n",
      "          0.9867,  0.2247,  0.9999,  0.9999, -1.0000,  0.8488,  0.9990,  0.8796,\n",
      "          0.9753, -0.9976, -0.6096,  0.5935, -0.8588,  0.9934, -0.3182, -0.7354,\n",
      "          0.9336, -0.9997,  0.9305, -0.7259, -0.3933,  0.9306, -0.9930,  0.4926,\n",
      "         -0.9929, -0.0789, -1.0000, -0.9935, -0.9999, -0.5658,  0.9977, -0.7162,\n",
      "          1.0000,  0.3445, -0.7940, -0.5686, -1.0000,  0.9548,  0.6614, -0.7902,\n",
      "          0.4443, -0.9751,  0.9037, -0.9998, -1.0000,  0.7921, -0.7284,  0.8903,\n",
      "          0.9856,  0.9845, -0.8437,  0.9679, -0.8474,  0.8981, -0.8510, -0.9888,\n",
      "          0.9513,  0.3377,  1.0000, -0.9994, -0.9919, -0.9785, -0.9663, -0.9675,\n",
      "          0.8902, -0.0773,  0.8917, -0.3305,  0.8547,  0.9974, -0.9988, -0.9977,\n",
      "          1.0000, -0.9999,  0.9810,  0.1002, -0.3030, -0.8942, -0.4931, -0.2150,\n",
      "         -0.6841, -0.9019, -1.0000, -0.9980,  0.9957, -0.9972, -0.9690, -0.9499,\n",
      "         -1.0000,  0.9996,  0.9820,  1.0000, -1.0000,  0.8729,  0.6901,  0.9999,\n",
      "          0.2377, -0.7758, -0.9773,  1.0000,  0.9978, -0.9900,  0.5399, -0.8535,\n",
      "         -0.7624, -0.6028, -0.9294,  0.9789,  0.9897, -0.9996, -1.0000,  1.0000,\n",
      "         -0.5445,  0.9955,  0.2454, -0.8176, -0.7719, -0.9040, -0.1470, -0.9737,\n",
      "         -1.0000,  0.9135, -1.0000, -0.9976,  0.5079,  0.9964, -0.9996, -0.9796,\n",
      "          0.7931, -1.0000, -0.3561,  0.4034,  0.9236, -0.9941, -0.9028, -0.7326,\n",
      "          0.9991,  0.9388, -0.9875, -0.9927, -0.8240,  1.0000,  0.8569,  0.7576,\n",
      "          0.9874,  0.9363,  0.9090, -0.9678, -0.9997, -0.9971, -0.6112,  0.9990,\n",
      "          0.9974, -1.0000, -0.9997, -0.6638, -0.9783,  0.9931, -0.9233, -1.0000,\n",
      "         -1.0000,  0.6973, -0.8624,  0.9956, -0.4581,  1.0000,  0.9217,  0.9900,\n",
      "         -0.9059,  0.9706,  0.4372,  0.9840, -0.9331,  1.0000,  0.9938,  0.9936]],\n",
      "       device='cuda:0')\n",
      "  Accuracy: 1.000\n",
      "  Test Loss: 0.036\n",
      "  Test took: 0:00:00\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch    10  of     11.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss generetor: 0.777\n",
      "  Average training loss discriminator: 0.855\n",
      "  Training epcoh took: 0:00:04\n",
      "\n",
      "Running Test...\n",
      "Evaluation tensor([[-0.9502,  0.9155,  1.0000, -0.9978,  0.8874, -0.9773,  0.9835,  0.9198,\n",
      "         -0.9241, -0.0039,  0.9935,  0.9992,  0.9911, -0.9998, -0.9973, -0.9711,\n",
      "          0.9831, -0.9752, -1.0000,  0.9941,  0.8357, -1.0000,  0.3512, -0.7455,\n",
      "          0.9172,  0.5219,  0.9961,  1.0000,  0.7631,  0.9994,  0.7209, -0.9927,\n",
      "         -0.9746, -1.0000,  0.6863, -0.6587, -0.9927, -0.8750, -0.9623,  0.7672,\n",
      "         -0.8508,  0.9871, -0.9876, -0.8372, -0.9931,  0.7730,  0.9209,  0.2496,\n",
      "         -0.7328,  1.0000, -0.9861,  1.0000,  0.9439,  1.0000,  0.9965,  0.7684,\n",
      "          0.9949,  0.7851,  0.9548,  0.9855,  0.7796, -0.8317,  0.9789, -0.8387,\n",
      "         -0.9987, -0.9512,  0.9910,  0.8371, -0.7900,  0.9319,  0.6268,  0.7664,\n",
      "          0.9991, -0.9979, -0.7760, -0.9859,  0.9973, -1.0000,  0.9908,  1.0000,\n",
      "         -0.9950, -0.9999,  0.9976, -0.8883,  0.9964, -1.0000,  0.9549, -1.0000,\n",
      "          0.8196, -0.6278, -0.7344, -0.9979, -0.3059,  0.9758,  1.0000,  0.9835,\n",
      "         -0.8991,  0.7859, -0.9164,  0.9856, -0.9153, -0.9913, -0.9031,  0.9659,\n",
      "         -0.4967, -0.9991, -0.9213,  0.9526, -1.0000,  0.8362,  0.9800, -0.9647,\n",
      "          0.9942,  0.9236, -0.9828, -0.2322,  0.9906, -0.9010, -0.9531,  1.0000,\n",
      "          0.2489, -0.7952,  0.9996, -0.9956,  0.9632, -0.8373,  0.9950, -0.9855,\n",
      "          0.9987, -0.8759,  0.9431, -0.9965,  0.8732,  0.9999, -0.8350,  1.0000,\n",
      "         -0.9996, -0.9432, -1.0000,  0.7906,  0.7015,  0.1643,  0.9126,  0.9918,\n",
      "          0.9869,  0.8625,  0.9570,  0.9552, -0.9741,  0.9200,  0.9949,  0.9970,\n",
      "         -0.9262,  1.0000, -0.9607, -0.8873, -0.9826,  0.8899,  0.9748, -0.3232,\n",
      "          0.9953, -0.9996, -0.9995,  0.5937,  1.0000,  0.9904, -0.9796,  0.9952,\n",
      "          0.9999,  0.9999, -0.3699, -0.9956, -0.9822,  0.9772,  0.6359,  0.9409,\n",
      "          0.9480,  1.0000, -0.9923,  1.0000,  1.0000,  0.7500,  0.9152, -0.9178,\n",
      "         -0.9994, -0.9982, -0.9980,  0.9567, -0.9995, -0.9991, -0.7172,  0.9858,\n",
      "         -0.9912,  0.7910, -1.0000, -0.3977,  0.9968, -0.9108,  1.0000,  0.9995,\n",
      "         -1.0000,  0.8581, -0.9730,  0.2850, -0.8429,  0.8825,  0.6863,  0.8606,\n",
      "          0.4102, -1.0000, -0.9891, -0.6055, -0.9972,  0.3385,  0.9908,  0.9939,\n",
      "         -0.7764,  0.7810,  0.9719,  1.0000, -0.9999,  0.4046,  0.8912, -0.9998,\n",
      "         -0.9987,  0.9931, -0.6369,  0.9998, -0.8537, -1.0000,  0.6968, -0.9821,\n",
      "          0.9978,  0.9892,  0.9998, -1.0000,  0.9772, -0.8699,  0.1320,  0.7902,\n",
      "          0.6871, -0.9181, -0.9070,  0.9770,  0.9645,  0.1781,  0.3002, -0.9820,\n",
      "          0.9984,  0.8145, -0.9989,  0.9934, -0.9999,  0.9999,  0.9748, -0.7152,\n",
      "          0.9991, -0.9987, -0.0072,  0.9562, -0.2416, -1.0000, -0.9908, -1.0000,\n",
      "          0.6048, -0.3330,  0.9983, -0.7532,  0.9080, -0.9938, -1.0000,  0.9630,\n",
      "         -0.6352,  0.9996, -0.9934,  0.9998,  0.9971, -0.7961, -0.9976, -1.0000,\n",
      "         -0.9845,  1.0000, -0.9986, -0.7532,  1.0000,  0.9970, -0.8365, -0.9877,\n",
      "         -0.9990, -0.9998, -0.6073,  0.9757, -0.9083,  0.9708, -0.9960, -0.6852,\n",
      "          0.9996,  1.0000, -0.9033,  0.9758,  0.9129, -0.9840, -1.0000, -0.9995,\n",
      "          0.8806, -1.0000,  0.9999, -0.9904,  1.0000, -0.9907,  0.9900,  0.9331,\n",
      "         -0.7565,  0.9917,  0.7463,  1.0000,  0.9207, -0.6862,  0.7797, -0.4057,\n",
      "          0.8752, -0.9977,  0.9172,  0.9931,  0.5287, -0.9041, -0.7980, -0.9878,\n",
      "         -0.9916, -0.9610,  0.1547,  0.8461,  0.9889,  0.7935,  0.9837, -0.6876,\n",
      "          0.9891, -0.9427, -1.0000,  0.2322,  0.8716,  0.9185,  0.9899, -0.9999,\n",
      "          0.9941, -0.9640,  1.0000, -0.4274, -0.6277,  0.9371,  1.0000, -1.0000,\n",
      "          0.5352, -0.9821,  0.9399,  0.9978,  0.9996, -0.9415, -0.4514,  0.4276,\n",
      "          0.9784,  0.3817,  0.9933,  0.5295,  0.7630,  0.9837,  0.9998,  0.9995,\n",
      "         -0.9786, -0.8860, -0.9981,  0.8775,  0.7221,  0.9910,  0.8932, -0.7608,\n",
      "          0.5357, -0.8909, -0.9582,  0.9849, -0.4875,  0.3494, -0.9886, -0.8970,\n",
      "         -0.7239,  0.9999, -0.9988, -0.9936,  0.9959, -0.9392,  0.5125,  0.4607,\n",
      "         -0.9929, -0.7968, -0.6167, -0.9999,  0.7903, -0.9035,  0.1494,  0.9996,\n",
      "          0.5868,  0.4618, -0.9514, -0.8123,  0.9217,  0.9095,  0.8305,  0.9887,\n",
      "         -0.7612, -0.8189,  0.9836,  0.9017, -0.9160,  0.9760, -0.9403,  1.0000,\n",
      "         -0.6992, -1.0000,  0.9536,  0.9734, -1.0000, -0.9999, -1.0000,  0.9960,\n",
      "         -0.9946,  0.9930,  0.9923, -1.0000, -1.0000, -0.3689, -0.9982, -0.8520,\n",
      "         -0.9956,  1.0000,  0.6921,  0.9748, -0.4493, -0.9830,  0.9993,  0.9620,\n",
      "         -0.9888, -1.0000,  0.9948, -0.9350,  0.9030,  0.9987, -0.9970, -0.6910,\n",
      "          0.0515,  0.9595,  0.9680,  0.4384,  0.9999, -1.0000,  0.9978,  0.9848,\n",
      "          0.4830,  0.6640, -0.9913,  1.0000, -0.9926,  0.6195, -0.8308, -1.0000,\n",
      "         -0.9399,  0.9925,  0.9889, -0.9963,  0.6818, -0.8679, -1.0000,  0.9276,\n",
      "         -0.9905,  1.0000,  0.9887,  0.9578, -0.6903, -0.7712,  0.8981, -1.0000,\n",
      "         -0.9035, -0.9391, -0.8463,  0.9998,  0.9670,  0.9992,  0.9881,  0.5962,\n",
      "         -0.9991,  0.5755,  0.5098,  0.9999, -0.9336,  0.9244, -0.7123, -0.9912,\n",
      "         -0.9931,  0.9993,  0.9743,  0.8342, -0.6510, -0.8174,  0.9764, -0.9191,\n",
      "          0.4927, -1.0000, -0.9521, -0.8218, -1.0000,  1.0000, -1.0000, -0.5342,\n",
      "          1.0000, -0.9707,  0.9925,  0.7943, -1.0000, -1.0000, -0.8766, -0.9491,\n",
      "          0.9912, -0.8042,  0.4474,  0.9720,  0.9977,  0.9995,  1.0000,  1.0000,\n",
      "          0.9732,  0.9999,  0.3593, -0.9998, -0.7528, -1.0000,  0.2373,  0.8712,\n",
      "          0.9997,  0.9899,  0.9801,  1.0000, -1.0000,  1.0000, -1.0000,  0.9823,\n",
      "          1.0000, -0.9969,  0.8432, -0.9999,  0.9789, -0.9944,  0.9126, -0.8701,\n",
      "          0.9977, -1.0000, -0.9997, -0.9108,  0.8616,  0.9985, -0.8814,  0.9909,\n",
      "          0.9995,  0.4975,  0.8063, -0.8483, -0.9459,  1.0000,  0.8860,  0.9963,\n",
      "         -0.9973,  0.9795,  0.9918,  0.6941,  0.9734, -0.4067,  0.9863,  0.9241,\n",
      "         -0.9996,  0.4275, -1.0000, -0.0664, -0.8912,  0.8563,  0.8768,  0.9666,\n",
      "          0.8351, -0.9965,  0.4454, -0.9999,  0.9790,  0.9411,  0.8315, -0.8378,\n",
      "          0.9951,  0.6564,  0.9998,  0.9998, -1.0000,  0.8915,  0.9989,  0.9762,\n",
      "          0.9663, -0.9974, -0.5994,  0.7685, -0.8543,  0.9936, -0.2549, -0.7409,\n",
      "          0.8962, -0.9996,  0.9404, -0.7277, -0.4377,  0.9534, -0.9950,  0.5609,\n",
      "         -0.9965,  0.1255, -1.0000, -0.9982, -0.9998, -0.6326,  0.9984, -0.5046,\n",
      "          1.0000,  0.3091, -0.8630, -0.6594, -1.0000,  0.9852,  0.6362, -0.8248,\n",
      "          0.4445, -0.9908,  0.9486, -0.9999, -1.0000,  0.8635, -0.8807,  0.8967,\n",
      "          0.9819,  0.9950, -0.6357,  0.9690, -0.7430,  0.9075, -0.8564, -0.9804,\n",
      "          0.9521,  0.4045,  1.0000, -0.9993, -0.9866, -0.9537, -0.9893, -0.9754,\n",
      "          0.8851, -0.3554,  0.9511, -0.1994,  0.8326,  0.9972, -0.9992, -0.9967,\n",
      "          0.9999, -0.9999,  0.9899,  0.1561, -0.4778, -0.9266, -0.5725,  0.1588,\n",
      "         -0.5916, -0.9198, -1.0000, -0.9992,  0.9970, -0.9933, -0.9776, -0.9720,\n",
      "         -1.0000,  0.9996,  0.9843,  1.0000, -1.0000,  0.8820,  0.6805,  0.9999,\n",
      "          0.2849, -0.7805, -0.9850,  0.9999,  0.9993, -0.9945,  0.6132, -0.8875,\n",
      "         -0.8215, -0.8363, -0.9424,  0.9836,  0.9954, -0.9995, -1.0000,  1.0000,\n",
      "         -0.4474,  0.9951,  0.4357, -0.8869, -0.6451, -0.8778, -0.1568, -0.9464,\n",
      "         -1.0000,  0.9564, -1.0000, -0.9985,  0.5854,  0.9952, -0.9993, -0.9427,\n",
      "          0.8344, -1.0000, -0.1991,  0.5054,  0.9629, -0.9908, -0.9663, -0.7370,\n",
      "          0.9997,  0.8950, -0.9884, -0.9958, -0.8956,  1.0000,  0.8884,  0.8518,\n",
      "          0.9962,  0.9591,  0.9376, -0.9383, -0.9999, -0.9980, -0.1042,  0.9987,\n",
      "          0.9960, -0.9999, -0.9996, -0.7282, -0.9949,  0.9929, -0.9261, -1.0000,\n",
      "         -1.0000,  0.7582, -0.9221,  0.9955, -0.4543,  1.0000,  0.8773,  0.9964,\n",
      "         -0.9732,  0.9878,  0.5687,  0.9943, -0.9287,  1.0000,  0.9972,  0.9947]],\n",
      "       device='cuda:0')\n",
      "  Accuracy: 1.000\n",
      "  Test Loss: 0.014\n",
      "  Test took: 0:00:00\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    10  of     11.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss generetor: 0.722\n",
      "  Average training loss discriminator: 0.892\n",
      "  Training epcoh took: 0:00:04\n",
      "\n",
      "Running Test...\n",
      "Evaluation tensor([[-0.9267,  0.8891,  1.0000, -0.9946,  0.7959, -0.9762,  0.9569,  0.7718,\n",
      "         -0.8230,  0.2380,  0.9827,  0.9976,  0.9740, -0.9993, -0.9968, -0.9327,\n",
      "          0.9677, -0.9646, -1.0000,  0.9936,  0.7496, -1.0000,  0.2414, -0.7155,\n",
      "          0.7781,  0.5161,  0.9878,  1.0000,  0.4138,  0.9993,  0.6388, -0.9859,\n",
      "         -0.9709, -0.9999,  0.6213, -0.6095, -0.9913, -0.8205, -0.9716,  0.8380,\n",
      "         -0.6895,  0.9879, -0.9832, -0.7869, -0.9911,  0.6478,  0.9147,  0.1459,\n",
      "         -0.6455,  1.0000, -0.9587,  1.0000,  0.8618,  1.0000,  0.9908,  0.7157,\n",
      "          0.9877,  0.7478,  0.9086,  0.9772,  0.5953, -0.7342,  0.9548, -0.8008,\n",
      "         -0.9982, -0.8873,  0.9893,  0.8068, -0.6242,  0.8681,  0.4570,  0.7181,\n",
      "          0.9976, -0.9956, -0.7238, -0.9771,  0.9961, -1.0000,  0.9810,  1.0000,\n",
      "         -0.9906, -0.9997,  0.9938, -0.8431,  0.9948, -0.9999,  0.9307, -0.9999,\n",
      "          0.7625, -0.4987, -0.6021, -0.9941, -0.3424,  0.9811,  1.0000,  0.9708,\n",
      "         -0.8535,  0.7690, -0.8724,  0.9558, -0.8332, -0.9874, -0.8138,  0.9340,\n",
      "         -0.2620, -0.9987, -0.8247,  0.9438, -1.0000,  0.8090,  0.9500, -0.9005,\n",
      "          0.9915,  0.9000, -0.9826, -0.2616,  0.9801, -0.9048, -0.9347,  1.0000,\n",
      "          0.3771, -0.8584,  0.9990, -0.9962,  0.9169, -0.7677,  0.9949, -0.9845,\n",
      "          0.9984, -0.7955,  0.9230, -0.9907,  0.8236,  0.9998, -0.7854,  1.0000,\n",
      "         -0.9990, -0.9161, -1.0000,  0.4672,  0.6532,  0.1685,  0.9041,  0.9908,\n",
      "          0.9582,  0.8001,  0.9343,  0.9100, -0.9413,  0.8469,  0.9925,  0.9945,\n",
      "         -0.8745,  1.0000, -0.9246, -0.8494, -0.9848,  0.8627,  0.9708,  0.0114,\n",
      "          0.9822, -0.9992, -0.9996,  0.4922,  0.9999,  0.9771, -0.9782,  0.9859,\n",
      "          0.9998,  0.9998, -0.4254, -0.9953, -0.9702,  0.9533,  0.5562,  0.9132,\n",
      "          0.9134,  1.0000, -0.9878,  0.9999,  1.0000,  0.7812,  0.8672, -0.8453,\n",
      "         -0.9983, -0.9942, -0.9954,  0.9630, -0.9989, -0.9991, -0.6024,  0.9751,\n",
      "         -0.9807,  0.5653, -0.9999, -0.2649,  0.9922, -0.8953,  1.0000,  0.9993,\n",
      "         -0.9999,  0.9189, -0.9742, -0.2055, -0.8256,  0.6260,  0.5589,  0.8275,\n",
      "          0.6639, -1.0000, -0.9750, -0.5125, -0.9968, -0.0507,  0.9804,  0.9928,\n",
      "         -0.7670,  0.7059,  0.9632,  1.0000, -0.9998,  0.5120,  0.8044, -0.9994,\n",
      "         -0.9962,  0.9839, -0.5466,  0.9998, -0.8169, -1.0000,  0.6657, -0.9696,\n",
      "          0.9957,  0.9842,  0.9995, -0.9999,  0.9530, -0.6693, -0.0181,  0.7313,\n",
      "          0.5222, -0.8984, -0.7145,  0.9599,  0.9313,  0.3263,  0.1720, -0.9694,\n",
      "          0.9979,  0.7414, -0.9989,  0.9941, -0.9998,  0.9998,  0.9683, -0.6547,\n",
      "          0.9977, -0.9963,  0.3580,  0.9067, -0.2606, -1.0000, -0.9868, -0.9999,\n",
      "          0.4855, -0.1286,  0.9955, -0.4819,  0.8708, -0.9917, -1.0000,  0.9133,\n",
      "         -0.4743,  0.9991, -0.9925,  0.9996,  0.9930, -0.6098, -0.9948, -1.0000,\n",
      "         -0.9766,  1.0000, -0.9963, -0.6578,  0.9999,  0.9943, -0.6793, -0.9809,\n",
      "         -0.9970, -0.9996, -0.5669,  0.9697, -0.8810,  0.9189, -0.9955, -0.6279,\n",
      "          0.9989,  1.0000, -0.8783,  0.9710,  0.8800, -0.9668, -1.0000, -0.9997,\n",
      "          0.8547, -1.0000,  0.9998, -0.9725,  1.0000, -0.9882,  0.9713,  0.8726,\n",
      "         -0.6938,  0.9874,  0.7198,  1.0000,  0.8631, -0.5945,  0.6894, -0.2320,\n",
      "          0.8615, -0.9958,  0.8683,  0.9907,  0.4922, -0.8316, -0.7412, -0.9746,\n",
      "         -0.9782, -0.9354,  0.0187,  0.6888,  0.9814,  0.5537,  0.9665, -0.6115,\n",
      "          0.9811, -0.9266, -1.0000, -0.0298,  0.8194,  0.8240,  0.9866, -0.9999,\n",
      "          0.9892, -0.9417,  1.0000, -0.4046, -0.4870,  0.8954,  1.0000, -0.9998,\n",
      "          0.4053, -0.9731,  0.9147,  0.9967,  0.9990, -0.8457, -0.0402,  0.4643,\n",
      "          0.9679,  0.0428,  0.9835,  0.4680,  0.7339,  0.9645,  0.9997,  0.9988,\n",
      "         -0.9451, -0.8508, -0.9974,  0.7688,  0.3959,  0.9862,  0.8573, -0.7296,\n",
      "          0.4147, -0.8266, -0.9317,  0.9758, -0.3917,  0.2982, -0.9664, -0.7131,\n",
      "         -0.6240,  0.9997, -0.9966, -0.9898,  0.9913, -0.9167,  0.2163,  0.2536,\n",
      "         -0.9798, -0.7085, -0.4811, -0.9998,  0.7579, -0.8607,  0.2223,  0.9997,\n",
      "          0.5148,  0.2920, -0.9335, -0.7154,  0.8524,  0.9030,  0.5276,  0.9791,\n",
      "         -0.7075, -0.7589,  0.9792,  0.8582, -0.8835,  0.9528, -0.8364,  1.0000,\n",
      "         -0.4822, -1.0000,  0.9061,  0.9617, -1.0000, -0.9999, -1.0000,  0.9922,\n",
      "         -0.9947,  0.9870,  0.9844, -1.0000, -1.0000, -0.1218, -0.9981, -0.8054,\n",
      "         -0.9948,  1.0000,  0.6907,  0.9702, -0.2567, -0.9704,  0.9991,  0.9376,\n",
      "         -0.9844, -1.0000,  0.9944, -0.8510,  0.8033,  0.9985, -0.9946, -0.8471,\n",
      "          0.2450,  0.9307,  0.9168,  0.4276,  0.9998, -1.0000,  0.9934,  0.9737,\n",
      "          0.3591,  0.6144, -0.9824,  0.9999, -0.9903,  0.5428, -0.7788, -1.0000,\n",
      "         -0.9347,  0.9882,  0.9840, -0.9906,  0.6204, -0.6963, -1.0000,  0.9051,\n",
      "         -0.9848,  0.9999,  0.9780,  0.9403, -0.6096, -0.5689,  0.8717, -1.0000,\n",
      "         -0.8560, -0.8868, -0.6088,  0.9997,  0.9096,  0.9979,  0.9858,  0.6780,\n",
      "         -0.9986,  0.4908,  0.1001,  0.9999, -0.8993,  0.8888, -0.6819, -0.9748,\n",
      "         -0.9846,  0.9984,  0.9524,  0.6320, -0.5809, -0.6517,  0.9756, -0.9062,\n",
      "          0.5867, -1.0000, -0.9254, -0.7618, -0.9999,  0.9999, -1.0000, -0.2592,\n",
      "          1.0000, -0.9714,  0.9828,  0.7130, -1.0000, -1.0000, -0.8384, -0.9432,\n",
      "          0.9831, -0.7333,  0.3211,  0.9555,  0.9940,  0.9986,  1.0000,  1.0000,\n",
      "          0.9462,  0.9998,  0.0798, -0.9996, -0.6641, -0.9999,  0.5753,  0.7910,\n",
      "          0.9993,  0.9753,  0.9595,  1.0000, -1.0000,  0.9998, -1.0000,  0.9600,\n",
      "          0.9999, -0.9923,  0.6898, -0.9998,  0.9604, -0.9923,  0.8816, -0.8343,\n",
      "          0.9956, -0.9999, -0.9992, -0.9496,  0.8227,  0.9975, -0.8339,  0.9908,\n",
      "          0.9988,  0.3301,  0.7265, -0.8064, -0.8742,  1.0000,  0.8837,  0.9960,\n",
      "         -0.9935,  0.9462,  0.9860,  0.5907,  0.9445, -0.2439,  0.9793,  0.9078,\n",
      "         -0.9992,  0.4379, -1.0000,  0.4634, -0.9120,  0.8012,  0.8273,  0.9503,\n",
      "          0.8111, -0.9929,  0.2662, -0.9997,  0.9367,  0.9509,  0.7676, -0.7847,\n",
      "          0.9933,  0.7818,  0.9995,  0.9995, -1.0000,  0.8695,  0.9968,  0.9752,\n",
      "          0.9332, -0.9936, -0.4960,  0.8321, -0.7064,  0.9860, -0.3513, -0.6863,\n",
      "          0.8491, -0.9988,  0.8871, -0.5821, -0.2287,  0.9301, -0.9894,  0.4422,\n",
      "         -0.9928,  0.0863, -0.9998, -0.9974, -0.9993, -0.4975,  0.9956, -0.3944,\n",
      "          0.9999,  0.0870, -0.8428, -0.6355, -0.9999,  0.9603,  0.5238, -0.7866,\n",
      "          0.1583, -0.9780,  0.9389, -0.9999, -1.0000,  0.8480, -0.8372,  0.8596,\n",
      "          0.9533,  0.9900, -0.2877,  0.9622, -0.4650,  0.8717, -0.8007, -0.9655,\n",
      "          0.9230,  0.3419,  1.0000, -0.9981, -0.9696, -0.9203, -0.9879, -0.9660,\n",
      "          0.8297, -0.3243,  0.9465,  0.2118,  0.8241,  0.9941, -0.9982, -0.9931,\n",
      "          0.9999, -0.9999,  0.9818,  0.3444, -0.4388, -0.8990, -0.4423,  0.4816,\n",
      "         -0.4358, -0.8823, -1.0000, -0.9993,  0.9957, -0.9746, -0.9586, -0.9621,\n",
      "         -1.0000,  0.9990,  0.9625,  1.0000, -0.9999,  0.7818,  0.5889,  0.9997,\n",
      "          0.2843, -0.7511, -0.9834,  0.9998,  0.9991, -0.9925,  0.6153, -0.8464,\n",
      "         -0.7261, -0.8296, -0.8210,  0.9730,  0.9937, -0.9984, -1.0000,  1.0000,\n",
      "         -0.3120,  0.9885,  0.3116, -0.8058, -0.3013, -0.7827, -0.0985, -0.8726,\n",
      "         -1.0000,  0.9292, -1.0000, -0.9969,  0.5282,  0.9851, -0.9985, -0.8284,\n",
      "          0.7556, -1.0000, -0.1733,  0.5145,  0.9308, -0.9778, -0.9466, -0.5614,\n",
      "          0.9995,  0.8148, -0.9765, -0.9941, -0.9223,  1.0000,  0.8598,  0.8333,\n",
      "          0.9952,  0.9177,  0.9214, -0.8801, -0.9999, -0.9953,  0.3619,  0.9968,\n",
      "          0.9875, -0.9998, -0.9991, -0.6364, -0.9934,  0.9883, -0.8782, -0.9999,\n",
      "         -1.0000,  0.7160, -0.8715,  0.9917, -0.3648,  1.0000,  0.7838,  0.9955,\n",
      "         -0.9632,  0.9822,  0.4258,  0.9906, -0.9047,  1.0000,  0.9948,  0.9914]],\n",
      "       device='cuda:0')\n",
      "  Accuracy: 1.000\n",
      "  Test Loss: 0.014\n",
      "  Test took: 0:00:00\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch    10  of     11.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss generetor: 0.743\n",
      "  Average training loss discriminator: 0.799\n",
      "  Training epcoh took: 0:00:04\n",
      "\n",
      "Running Test...\n",
      "Evaluation tensor([[-0.8218,  0.7613,  0.9999, -0.9864,  0.6163, -0.7007,  0.9324, -0.9177,\n",
      "         -0.6316,  0.5124,  0.9684,  0.9947, -0.5951, -0.9978, -0.9237, -0.7791,\n",
      "          0.9348, -0.9360, -0.9999,  0.9341, -0.3414, -0.9999, -0.0025,  0.5039,\n",
      "          0.5471,  0.5275,  0.9686,  1.0000,  0.6259,  0.9750,  0.5230, -0.9732,\n",
      "         -0.6567, -0.9997,  0.3845, -0.1071, -0.8436, -0.7218, -0.4497,  0.0269,\n",
      "         -0.3388,  0.7179, -0.8442, -0.5885, -0.6267,  0.3467,  0.7313, -0.0430,\n",
      "         -0.2062,  1.0000, -0.8741,  1.0000, -0.8714,  0.9986,  0.9626,  0.4911,\n",
      "          0.9856,  0.6333, -0.9247,  0.7873,  0.4186, -0.4637,  0.8865, -0.0934,\n",
      "         -0.9489, -0.5244,  0.8185,  0.6819, -0.1861,  0.7394, -0.0772,  0.4201,\n",
      "          0.9844, -0.9902, -0.4915, -0.9672,  0.9556, -0.9998,  0.9543,  0.9999,\n",
      "         -0.6376, -0.9989,  0.9806, -0.6442,  0.7931, -0.9850, -0.8308, -0.9998,\n",
      "          0.6118, -0.2492,  0.4143, -0.9723,  0.5641,  0.2918,  1.0000,  0.5453,\n",
      "         -0.7208,  0.6689, -0.0191,  0.1221, -0.5619, -0.6200,  0.9284, -0.8013,\n",
      "          0.9749, -0.9539, -0.7226,  0.4497, -0.9993,  0.6589,  0.8996, -0.7328,\n",
      "          0.7958,  0.6574, -0.6687, -0.8203,  0.9661, -0.4566, -0.8736,  1.0000,\n",
      "          0.5379, -0.1598,  0.9972, -0.9286, -0.0954, -0.6214,  0.8238, -0.7158,\n",
      "          0.9485, -0.0101,  0.8505, -0.9826, -0.8289,  0.9997, -0.5485,  1.0000,\n",
      "         -0.9954,  0.5184, -0.9999, -0.7338, -0.4330,  0.1792, -0.1892,  0.8539,\n",
      "          0.9371,  0.4571,  0.1685, -0.2270, -0.5951,  0.1830,  0.9593,  0.9845,\n",
      "         -0.7977,  1.0000,  0.8085,  0.2369, -0.6904,  0.7778,  0.5193,  0.8329,\n",
      "          0.9444, -0.9973, -0.9881, -0.9392,  0.9997,  0.9537, -0.6223, -0.5637,\n",
      "          0.9994,  0.9897, -0.6776, -0.9450, -0.8504, -0.7563,  0.4441,  0.8491,\n",
      "          0.7053,  0.9999, -0.9701,  0.9997,  1.0000,  0.9046,  0.5627,  0.9536,\n",
      "         -0.9940, -0.9807, -0.9851,  0.8060, -0.9109, -0.9740,  0.2424,  0.9252,\n",
      "          0.5751,  0.0095, -0.9997, -0.0146,  0.9631, -0.7777,  1.0000,  0.9786,\n",
      "         -0.9999,  0.3000, -0.4775, -0.0609, -0.6663,  0.6453, -0.5723,  0.2474,\n",
      "          0.9834, -1.0000,  0.5856,  0.3787, -0.8358,  0.0839,  0.9345,  0.7380,\n",
      "         -0.4813,  0.4941,  0.7006,  0.9999, -0.9990,  0.6641,  0.4400, -0.9987,\n",
      "         -0.9917,  0.9704, -0.2881,  0.9913, -0.7601, -0.9944,  0.4629, -0.4862,\n",
      "          0.9856,  0.8038,  0.9602, -0.9999, -0.7956, -0.5496, -0.9103,  0.4036,\n",
      "          0.3846, -0.7430, -0.6335, -0.7720,  0.8884,  0.8734, -0.7576, -0.8126,\n",
      "          0.9102, -0.9114, -0.9707,  0.8905, -0.9996,  0.9996,  0.1794,  0.8958,\n",
      "          0.9642, -0.9892,  0.8978, -0.8759, -0.2446, -1.0000, -0.8918, -0.9888,\n",
      "         -0.6632,  0.1563,  0.9819, -0.6435,  0.1316, -0.8994, -0.9999,  0.6953,\n",
      "         -0.3232,  0.9988, -0.8936,  0.9917,  0.9524,  0.4387, -0.9817, -0.9999,\n",
      "         -0.6484,  1.0000, -0.9925, -0.4229,  0.9996, -0.2155, -0.1904, -0.9387,\n",
      "         -0.9847, -0.9992,  0.1854,  0.3334, -0.7822,  0.7591, -0.9263, -0.1611,\n",
      "          0.9950,  0.9944, -0.6672,  0.7170,  0.7379, -0.9614, -1.0000, -0.9724,\n",
      "          0.6578, -1.0000,  0.9996, -0.9242,  1.0000, -0.7132, -0.6544,  0.8422,\n",
      "         -0.4720,  0.6037,  0.5479,  0.9999,  0.8576, -0.3807,  0.4436,  0.5376,\n",
      "          0.5787, -0.8734, -0.2686,  0.8236,  0.3680, -0.7187,  0.7941, -0.5876,\n",
      "         -0.9458,  0.6741, -0.4423,  0.4142,  0.6661,  0.6641,  0.9535, -0.2838,\n",
      "          0.6844, -0.7287, -0.9999, -0.9508,  0.4299, -0.8411,  0.8361, -0.9980,\n",
      "          0.9522, -0.9345,  0.9987, -0.1145,  0.5583, -0.8741,  1.0000, -0.9880,\n",
      "          0.2133, -0.5768,  0.4189,  0.9056,  0.9976,  0.7370,  0.9036,  0.4270,\n",
      "          0.7091,  0.2162,  0.8896, -0.6402,  0.6139, -0.8572,  0.9836,  0.9974,\n",
      "          0.8675, -0.7579, -0.9619, -0.9216,  0.3772,  0.8111,  0.0436, -0.3651,\n",
      "         -0.7112,  0.2287,  0.7346,  0.4190,  0.4386,  0.0736, -0.9112,  0.4894,\n",
      "          0.3279,  0.9990, -0.9893, -0.9067,  0.9790, -0.8357, -0.8194,  0.0687,\n",
      "          0.7316, -0.7363, -0.2623, -0.9992,  0.7017, -0.7431, -0.1189,  0.9781,\n",
      "          0.5209, -0.7917, -0.1140,  0.0347,  0.7452,  0.6120,  0.7266,  0.8998,\n",
      "         -0.4773, -0.5035,  0.8212,  0.6476, -0.5428,  0.8414, -0.7460,  1.0000,\n",
      "          0.0753, -1.0000, -0.8947,  0.0255, -0.9999, -0.9928, -0.9947,  0.9807,\n",
      "         -0.8259, -0.4280, -0.5762, -0.9949, -0.9999,  0.7166, -0.9642, -0.4868,\n",
      "         -0.9005,  0.9999,  0.4092,  0.8249, -0.1673, -0.9560,  0.9743, -0.8354,\n",
      "         -0.6557, -0.9999,  0.7686,  0.8579, -0.9049,  0.9743, -0.9703, -0.9799,\n",
      "          0.4103,  0.8105,  0.8671, -0.2130,  0.9900, -0.9999,  0.9698,  0.6219,\n",
      "          0.0415, -0.3599, -0.9573,  0.9998, -0.7873,  0.1956, -0.5471, -0.9999,\n",
      "          0.1670,  0.8062,  0.7671, -0.9748,  0.3762, -0.7193, -1.0000,  0.7232,\n",
      "          0.1067,  0.9898,  0.9759,  0.7025, -0.4498, -0.3657,  0.7760, -0.9999,\n",
      "          0.3051,  0.2947, -0.6102,  0.9729,  0.8115,  0.9929,  0.6410, -0.4535,\n",
      "         -0.8988,  0.1417,  0.3869,  0.9784, -0.6588,  0.7555, -0.5660, -0.9495,\n",
      "         -0.9711,  0.9947, -0.8360,  0.5651,  0.8484,  0.9577,  0.8306, -0.7196,\n",
      "         -0.6146, -0.9996, -0.8471, -0.2598, -0.9998,  0.9997, -0.9999,  0.5976,\n",
      "          0.9962, -0.5247,  0.9760,  0.2845, -1.0000, -0.9999,  0.0526, -0.7572,\n",
      "          0.9665, -0.2613,  0.1718,  0.3900,  0.9484,  0.9959,  0.9997,  0.9995,\n",
      "         -0.8029,  0.9992, -0.3123, -0.9993,  0.8338, -0.9995,  0.9505,  0.5754,\n",
      "          0.9962,  0.8901, -0.8020,  1.0000, -0.9999,  0.9901, -1.0000, -0.8426,\n",
      "          0.9995, -0.9908, -0.3139, -0.9996, -0.5825, -0.7947,  0.6880, -0.6567,\n",
      "          0.9856, -0.9999, -0.9979, -0.9721, -0.1002,  0.9115,  0.8261,  0.7145,\n",
      "          0.9918, -0.2694,  0.7795, -0.4180,  0.8516,  0.9999,  0.2292,  0.9669,\n",
      "         -0.9806,  0.9411,  0.7447,  0.4175,  0.8820,  0.0252,  0.3819,  0.7484,\n",
      "         -0.9987,  0.7407, -0.9974,  0.9689, -0.0761,  0.6604,  0.4823,  0.6424,\n",
      "          0.5881, -0.9821,  0.2814, -0.9994,  0.9273,  0.4811,  0.4563, -0.5345,\n",
      "          0.9335,  0.6561,  0.9990,  0.9986, -1.0000,  0.6057,  0.9931,  0.4104,\n",
      "          0.9008, -0.9811, -0.1174,  0.9759, -0.5023,  0.9570, -0.6379, -0.5095,\n",
      "          0.7751, -0.9966,  0.0284, -0.2457,  0.4319,  0.6530, -0.9724, -0.0766,\n",
      "         -0.4468, -0.6263, -0.9996, -0.9306, -0.9988, -0.0897,  0.9867, -0.9088,\n",
      "          0.9998, -0.7786, -0.6411, -0.3357, -0.9996, -0.8471,  0.2635, -0.6116,\n",
      "         -0.6529,  0.7027,  0.8082, -0.9942, -1.0000,  0.5814,  0.8564,  0.7320,\n",
      "          0.9082,  0.6739, -0.2670,  0.6196, -0.1502,  0.6845,  0.3656, -0.9438,\n",
      "          0.4121, -0.6874,  0.9999, -0.9938, -0.9354, -0.9308, -0.8154, -0.5842,\n",
      "          0.7557, -0.1063,  0.3488,  0.9422,  0.1322,  0.9847, -0.9937, -0.9909,\n",
      "          0.9997, -0.9960, -0.0778,  0.6543, -0.0713, -0.5730,  0.1099,  0.9039,\n",
      "         -0.5892, -0.7240, -0.9936, -0.9665,  0.8855, -0.9355, -0.8240, -0.8613,\n",
      "         -1.0000,  0.9970,  0.9132,  1.0000, -0.9997,  0.3425,  0.4220,  0.9995,\n",
      "          0.2041, -0.5458, -0.7341,  0.9992,  0.9497, -0.7053,  0.4356, -0.6994,\n",
      "         -0.0961, -0.2743,  0.9157,  0.6894,  0.9265, -0.9968, -1.0000,  0.9998,\n",
      "         -0.0114,  0.9796, -0.1888, -0.1334, -0.0856,  0.4312, -0.9065, -0.8273,\n",
      "         -0.9999,  0.5674, -1.0000, -0.9843,  0.1396,  0.9521, -0.9962, -0.6710,\n",
      "          0.2645, -1.0000,  0.7432, -0.8038, -0.2188, -0.9685,  0.6924, -0.1722,\n",
      "          0.9279,  0.7565, -0.8911, -0.7560, -0.4363,  0.9999,  0.6550,  0.5730,\n",
      "          0.8961, -0.6184,  0.1903, -0.8351, -0.9976, -0.9750,  0.0039,  0.9913,\n",
      "          0.9751, -0.9991, -0.9981,  0.8650, -0.8796,  0.9781, -0.7738, -0.9999,\n",
      "         -0.9999,  0.4846, -0.4988,  0.9751, -0.1407,  1.0000,  0.6812,  0.9216,\n",
      "         -0.4297,  0.6465, -0.0691,  0.8589, -0.7600,  0.9999,  0.8373,  0.9690]],\n",
      "       device='cuda:0')\n",
      "  Accuracy: 1.000\n",
      "  Test Loss: 0.040\n",
      "  Test took: 0:00:00\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    10  of     11.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss generetor: 0.732\n",
      "  Average training loss discriminator: 0.792\n",
      "  Training epcoh took: 0:00:04\n",
      "\n",
      "Running Test...\n",
      "Evaluation tensor([[-0.9411,  0.8777,  1.0000, -0.9971,  0.8703, -0.9706,  0.9735,  0.6175,\n",
      "         -0.9138,  0.3210,  0.9904,  0.9985,  0.9367, -0.9997, -0.9962, -0.9519,\n",
      "          0.9799, -0.9644, -1.0000,  0.9849,  0.5631, -1.0000,  0.2443, -0.6994,\n",
      "          0.8990,  0.5663,  0.9911,  1.0000,  0.7015,  0.9989,  0.6387, -0.9898,\n",
      "         -0.9750, -0.9999,  0.6598, -0.4560, -0.9933, -0.7994, -0.9635,  0.7570,\n",
      "         -0.8137,  0.9788, -0.9746, -0.7479, -0.9591,  0.7021,  0.8957,  0.1449,\n",
      "         -0.6121,  1.0000, -0.9776,  1.0000,  0.6270,  1.0000,  0.9932,  0.7632,\n",
      "          0.9934,  0.7547,  0.8071,  0.9629,  0.7189, -0.6335,  0.9762, -0.6724,\n",
      "         -0.9974, -0.8926,  0.9853,  0.7700, -0.6093,  0.8182,  0.6151,  0.6421,\n",
      "          0.9978, -0.9960, -0.7114, -0.9852,  0.9957, -1.0000,  0.9873,  1.0000,\n",
      "         -0.9867, -0.9998,  0.9963, -0.8307,  0.9915, -0.9998,  0.8428, -0.9999,\n",
      "          0.6638, -0.6334, -0.5381, -0.9957, -0.3860,  0.9624,  1.0000,  0.9279,\n",
      "         -0.8644,  0.8113, -0.8323,  0.9056, -0.7873, -0.9789, -0.6554,  0.8300,\n",
      "          0.2154, -0.9987, -0.7878,  0.8961, -1.0000,  0.8013,  0.9709, -0.9573,\n",
      "          0.9865,  0.8622, -0.9565,  0.0709,  0.9916, -0.8816, -0.9136,  1.0000,\n",
      "          0.3288, -0.8574,  0.9995, -0.9947,  0.8858, -0.6835,  0.9924, -0.9535,\n",
      "          0.9961, -0.6020,  0.9237, -0.9938,  0.7153,  0.9999, -0.7350,  1.0000,\n",
      "         -0.9993, -0.8293, -1.0000,  0.2619,  0.5613,  0.2068,  0.8507,  0.9885,\n",
      "          0.9787,  0.7308,  0.8998,  0.7305, -0.9108,  0.9043,  0.9888,  0.9928,\n",
      "         -0.9142,  1.0000, -0.8499, -0.7554, -0.9647,  0.8420,  0.9591,  0.1308,\n",
      "          0.9904, -0.9995, -0.9984,  0.1438,  1.0000,  0.9773, -0.9647,  0.9622,\n",
      "          0.9999,  0.9997, -0.0610, -0.9957, -0.9475,  0.9090,  0.5151,  0.9198,\n",
      "          0.8752,  1.0000, -0.9912,  1.0000,  1.0000,  0.8156,  0.8315, -0.6630,\n",
      "         -0.9988, -0.9953, -0.9955,  0.9563, -0.9986, -0.9969, -0.5382,  0.9792,\n",
      "         -0.9562,  0.5718, -1.0000, -0.1896,  0.9920, -0.8981,  1.0000,  0.9987,\n",
      "         -1.0000,  0.8491, -0.9575,  0.5253, -0.8293,  0.7843,  0.4642,  0.7731,\n",
      "          0.5424, -1.0000, -0.9618, -0.3183, -0.9928,  0.2270,  0.9908,  0.9848,\n",
      "         -0.7320,  0.6136,  0.9519,  1.0000, -0.9999,  0.4523,  0.7945, -0.9996,\n",
      "         -0.9970,  0.9924, -0.5119,  0.9996, -0.8256, -1.0000,  0.7290, -0.9654,\n",
      "          0.9967,  0.9772,  0.9993, -1.0000,  0.9039, -0.7349, -0.0227,  0.6734,\n",
      "          0.4388, -0.8936, -0.8238,  0.9006,  0.9421,  0.4305,  0.4146, -0.9560,\n",
      "          0.9974,  0.6706, -0.9971,  0.9903, -0.9999,  0.9999,  0.9444, -0.5966,\n",
      "          0.9978, -0.9976, -0.0900,  0.8730, -0.2689, -1.0000, -0.9612, -0.9999,\n",
      "          0.3619, -0.0770,  0.9960, -0.8247,  0.8780, -0.9849, -1.0000,  0.9192,\n",
      "         -0.5047,  0.9994, -0.9877,  0.9995,  0.9954, -0.6783, -0.9959, -1.0000,\n",
      "         -0.9689,  1.0000, -0.9975, -0.6053,  1.0000,  0.9868, -0.6601, -0.9738,\n",
      "         -0.9984, -0.9998, -0.4506,  0.9637, -0.8921,  0.9405, -0.9931, -0.6090,\n",
      "          0.9994,  1.0000, -0.8354,  0.9488,  0.8106, -0.9779, -1.0000, -0.9990,\n",
      "          0.8091, -1.0000,  0.9999, -0.9872,  1.0000, -0.9639,  0.9589,  0.9145,\n",
      "         -0.6973,  0.9662,  0.7267,  1.0000,  0.9071, -0.5671,  0.6712, -0.3346,\n",
      "          0.7908, -0.9944,  0.8065,  0.9842,  0.4873, -0.9071, -0.4492, -0.9744,\n",
      "         -0.9846, -0.8349, -0.0273,  0.5996,  0.9808,  0.7500,  0.9804, -0.5583,\n",
      "          0.9613, -0.9054, -1.0000, -0.2046,  0.7785,  0.7037,  0.9783, -0.9998,\n",
      "          0.9945, -0.9831,  1.0000, -0.4681, -0.5910,  0.8307,  1.0000, -0.9998,\n",
      "          0.3416, -0.9518,  0.9414,  0.9967,  0.9993, -0.8115, -0.0919,  0.6050,\n",
      "          0.9392,  0.5510,  0.9832,  0.3394,  0.7080,  0.9178,  0.9994,  0.9991,\n",
      "         -0.8860, -0.8400, -0.9973,  0.6285,  0.5623,  0.9836,  0.7917, -0.6947,\n",
      "          0.4370, -0.7438, -0.9188,  0.9559, -0.3894,  0.2578, -0.9762, -0.7334,\n",
      "         -0.5801,  0.9998, -0.9970, -0.9767,  0.9935, -0.8995, -0.1012,  0.1994,\n",
      "         -0.9526, -0.8799, -0.4919, -0.9999,  0.7826, -0.8960,  0.4708,  0.9992,\n",
      "          0.4765,  0.3254, -0.9029, -0.6071,  0.8978,  0.7705,  0.8256,  0.9573,\n",
      "         -0.6466, -0.7802,  0.9734,  0.8742, -0.8652,  0.9727, -0.9350,  1.0000,\n",
      "         -0.4449, -1.0000,  0.6987,  0.9422, -1.0000, -0.9999, -1.0000,  0.9963,\n",
      "         -0.9830,  0.9694,  0.9692, -1.0000, -1.0000, -0.2003, -0.9964, -0.7538,\n",
      "         -0.9914,  1.0000,  0.6584,  0.9206, -0.3272, -0.9842,  0.9960,  0.8705,\n",
      "         -0.9635, -1.0000,  0.9882, -0.6299,  0.7113,  0.9975, -0.9953, -0.6718,\n",
      "          0.0723,  0.9238,  0.9498,  0.6025,  0.9995, -1.0000,  0.9958,  0.9806,\n",
      "          0.3525,  0.7422, -0.9877,  1.0000, -0.9885,  0.5314, -0.7275, -1.0000,\n",
      "         -0.8947,  0.9900,  0.9786, -0.9943,  0.6248, -0.8343, -1.0000,  0.8823,\n",
      "         -0.9694,  1.0000,  0.9856,  0.9118, -0.6136, -0.7528,  0.8743, -1.0000,\n",
      "         -0.8123, -0.8778, -0.8284,  0.9994,  0.9773,  0.9987,  0.9723,  0.4013,\n",
      "         -0.9939,  0.5464,  0.4756,  0.9997, -0.8693,  0.8751, -0.6919, -0.9867,\n",
      "         -0.9858,  0.9990,  0.9137,  0.8847, -0.3361, -0.3792,  0.9609, -0.8827,\n",
      "          0.5609, -1.0000, -0.9295, -0.7042, -1.0000,  1.0000, -1.0000, -0.2985,\n",
      "          0.9999, -0.9555,  0.9897,  0.6711, -1.0000, -1.0000, -0.8870, -0.9323,\n",
      "          0.9910, -0.6554,  0.3105,  0.9453,  0.9875,  0.9992,  1.0000,  1.0000,\n",
      "          0.8950,  0.9999,  0.2767, -0.9997, -0.6276, -1.0000,  0.2673,  0.8573,\n",
      "          0.9988,  0.9731,  0.9108,  1.0000, -1.0000,  0.9999, -1.0000,  0.9322,\n",
      "          0.9999, -0.9954,  0.6402, -0.9999,  0.9025, -0.9921,  0.8539, -0.8334,\n",
      "          0.9973, -1.0000, -0.9997, -0.9270,  0.7687,  0.9954, -0.6967,  0.9873,\n",
      "          0.9992,  0.1272,  0.8660, -0.8083, -0.7479,  1.0000,  0.8747,  0.9938,\n",
      "         -0.9935,  0.9785,  0.9795,  0.6551,  0.9523, -0.2697,  0.9319,  0.8929,\n",
      "         -0.9994,  0.4051, -1.0000,  0.4709, -0.8647,  0.8288,  0.8017,  0.9251,\n",
      "          0.7365, -0.9950,  0.3492, -0.9998,  0.9585,  0.9477,  0.7578, -0.7907,\n",
      "          0.9928,  0.4581,  0.9998,  0.9997, -1.0000,  0.8338,  0.9982,  0.9704,\n",
      "          0.9539, -0.9963, -0.3380,  0.7828, -0.7623,  0.9902, -0.3222, -0.6423,\n",
      "          0.8949, -0.9990,  0.9189, -0.5302, -0.1329,  0.8283, -0.9894,  0.4391,\n",
      "         -0.9846, -0.2971, -0.9999, -0.9926, -0.9997, -0.3857,  0.9961, -0.1095,\n",
      "          0.9999, -0.0098, -0.8264, -0.6180, -0.9999,  0.8947,  0.4365, -0.7731,\n",
      "          0.3920, -0.9436,  0.9326, -0.9998, -1.0000,  0.7922, -0.6285,  0.8630,\n",
      "          0.9608,  0.9880, -0.6519,  0.9512, -0.4712,  0.8211, -0.7901, -0.9854,\n",
      "          0.9329,  0.4605,  1.0000, -0.9984, -0.9604, -0.9653, -0.9778, -0.9544,\n",
      "          0.8298, -0.3361,  0.9257,  0.0891,  0.7309,  0.9965, -0.9986, -0.9970,\n",
      "          0.9999, -0.9999,  0.9675,  0.1200, -0.5012, -0.8833, -0.3092,  0.0389,\n",
      "         -0.6769, -0.8262, -1.0000, -0.9988,  0.9944, -0.9932, -0.9097, -0.9436,\n",
      "         -1.0000,  0.9993,  0.9726,  1.0000, -0.9999,  0.6831,  0.5783,  0.9998,\n",
      "          0.3057, -0.7811, -0.9757,  0.9999,  0.9981, -0.9891,  0.6601, -0.8086,\n",
      "         -0.6415, -0.8486, -0.5944,  0.9670,  0.9941, -0.9992, -1.0000,  1.0000,\n",
      "         -0.2193,  0.9889,  0.2525, -0.8772, -0.4627, -0.7322, -0.3613, -0.8881,\n",
      "         -1.0000,  0.9194, -1.0000, -0.9974,  0.5300,  0.9954, -0.9993, -0.9640,\n",
      "          0.5856, -1.0000, -0.3966,  0.1720,  0.8440, -0.9893, -0.9286, -0.3774,\n",
      "          0.9994,  0.9125, -0.9816, -0.9886, -0.9193,  1.0000,  0.8340,  0.8252,\n",
      "          0.9904,  0.7959,  0.8800, -0.8642, -0.9998, -0.9961, -0.0432,  0.9982,\n",
      "          0.9940, -0.9999, -0.9994, -0.4267, -0.9900,  0.9955, -0.8840, -1.0000,\n",
      "         -1.0000,  0.6741, -0.8357,  0.9950, -0.4103,  1.0000,  0.8091,  0.9929,\n",
      "         -0.9635,  0.9619,  0.4197,  0.9756, -0.8733,  1.0000,  0.9887,  0.9903]],\n",
      "       device='cuda:0')\n",
      "  Accuracy: 1.000\n",
      "  Test Loss: 0.014\n",
      "  Test took: 0:00:00\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "  Batch    10  of     11.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss generetor: 0.722\n",
      "  Average training loss discriminator: 0.776\n",
      "  Training epcoh took: 0:00:04\n",
      "\n",
      "Running Test...\n",
      "Evaluation tensor([[-9.5317e-01,  8.9344e-01,  9.9999e-01, -9.9838e-01,  9.0751e-01,\n",
      "         -9.5348e-01,  9.7586e-01,  6.1996e-01, -9.4149e-01,  3.2332e-01,\n",
      "          9.9357e-01,  9.9920e-01,  9.1544e-01, -9.9984e-01, -9.9557e-01,\n",
      "         -9.7076e-01,  9.8644e-01, -9.7055e-01, -9.9999e-01,  9.8263e-01,\n",
      "          5.9548e-01, -9.9999e-01,  3.0024e-01, -6.1664e-01,  9.3149e-01,\n",
      "          5.9219e-01,  9.9427e-01,  1.0000e+00,  7.2899e-01,  9.9921e-01,\n",
      "          6.5981e-01, -9.9246e-01, -9.7047e-01, -9.9995e-01,  7.3277e-01,\n",
      "         -4.8147e-01, -9.9384e-01, -8.2421e-01, -9.6757e-01,  6.6876e-01,\n",
      "         -8.5449e-01,  9.7554e-01, -9.7662e-01, -7.7561e-01, -9.4709e-01,\n",
      "          7.4601e-01,  9.1062e-01,  1.6955e-01, -6.7184e-01,  1.0000e+00,\n",
      "         -9.8538e-01,  1.0000e+00,  4.6958e-01,  1.0000e+00,  9.9666e-01,\n",
      "          8.2778e-01,  9.9599e-01,  7.7854e-01,  7.8241e-01,  9.7238e-01,\n",
      "          7.8823e-01, -6.3952e-01,  9.8583e-01, -6.6057e-01, -9.9847e-01,\n",
      "         -9.2541e-01,  9.8793e-01,  7.7853e-01, -6.4771e-01,  8.2483e-01,\n",
      "          6.8327e-01,  6.7292e-01,  9.9873e-01, -9.9674e-01, -7.6440e-01,\n",
      "         -9.8921e-01,  9.9682e-01, -9.9999e-01,  9.9148e-01,  9.9999e-01,\n",
      "         -9.8718e-01, -9.9992e-01,  9.9805e-01, -8.5759e-01,  9.9261e-01,\n",
      "         -9.9986e-01,  8.2133e-01, -9.9995e-01,  6.4998e-01, -6.6627e-01,\n",
      "         -4.4399e-01, -9.9770e-01, -3.8018e-01,  9.5548e-01,  1.0000e+00,\n",
      "          9.0288e-01, -8.8251e-01,  8.4064e-01, -7.6235e-01,  8.9872e-01,\n",
      "         -8.1112e-01, -9.7412e-01, -5.6939e-01,  7.7777e-01,  4.4430e-01,\n",
      "         -9.9906e-01, -7.6030e-01,  8.5583e-01, -9.9999e-01,  8.1338e-01,\n",
      "          9.8150e-01, -9.6878e-01,  9.8597e-01,  8.8048e-01, -9.3976e-01,\n",
      "          4.8548e-02,  9.9485e-01, -8.7471e-01, -9.2029e-01,  1.0000e+00,\n",
      "          2.8618e-01, -7.9143e-01,  9.9972e-01, -9.9303e-01,  8.6151e-01,\n",
      "         -6.7090e-01,  9.9319e-01, -9.2811e-01,  9.9686e-01, -6.7433e-01,\n",
      "          9.3556e-01, -9.9629e-01,  6.4916e-01,  9.9994e-01, -7.5041e-01,\n",
      "          1.0000e+00, -9.9964e-01, -7.5263e-01, -9.9999e-01,  2.8723e-01,\n",
      "          4.4604e-01,  3.2721e-01,  7.9031e-01,  9.9059e-01,  9.8675e-01,\n",
      "          7.3620e-01,  8.6638e-01,  6.7514e-01, -9.2350e-01,  8.6091e-01,\n",
      "          9.9022e-01,  9.9400e-01, -9.3446e-01,  1.0000e+00, -8.4136e-01,\n",
      "         -6.4463e-01, -9.4245e-01,  8.5236e-01,  9.3807e-01,  2.0898e-01,\n",
      "          9.9461e-01, -9.9976e-01, -9.9801e-01, -8.1332e-02,  9.9997e-01,\n",
      "          9.8157e-01, -9.5700e-01,  9.5461e-01,  9.9996e-01,  9.9980e-01,\n",
      "         -2.1444e-02, -9.9652e-01, -9.5457e-01,  8.8891e-01,  5.4511e-01,\n",
      "          9.2848e-01,  8.9501e-01,  1.0000e+00, -9.9511e-01,  9.9999e-01,\n",
      "          1.0000e+00,  8.3645e-01,  8.7589e-01, -5.8604e-01, -9.9941e-01,\n",
      "         -9.9727e-01, -9.9699e-01,  9.6655e-01, -9.9880e-01, -9.9666e-01,\n",
      "         -6.1033e-01,  9.8667e-01, -9.4819e-01,  6.4880e-01, -9.9998e-01,\n",
      "         -1.9402e-01,  9.9533e-01, -9.1425e-01,  1.0000e+00,  9.9915e-01,\n",
      "         -9.9998e-01,  8.2212e-01, -9.4048e-01,  6.3530e-01, -8.4703e-01,\n",
      "          7.9938e-01,  4.3907e-01,  8.0234e-01,  6.6802e-01, -1.0000e+00,\n",
      "         -9.5662e-01, -4.4797e-01, -9.9179e-01,  2.9295e-01,  9.9498e-01,\n",
      "          9.8319e-01, -7.5991e-01,  6.5228e-01,  9.4986e-01,  9.9999e-01,\n",
      "         -9.9993e-01,  4.0064e-01,  8.1997e-01, -9.9975e-01, -9.9838e-01,\n",
      "          9.9477e-01, -5.4764e-01,  9.9943e-01, -8.2050e-01, -9.9999e-01,\n",
      "          7.6556e-01, -9.5473e-01,  9.9813e-01,  9.7705e-01,  9.9941e-01,\n",
      "         -9.9998e-01,  8.9079e-01, -7.9120e-01, -2.2471e-01,  6.9344e-01,\n",
      "          4.5506e-01, -9.1268e-01, -8.5485e-01,  8.8974e-01,  9.5372e-01,\n",
      "          5.2439e-01,  3.2143e-01, -9.6896e-01,  9.9764e-01,  6.1506e-01,\n",
      "         -9.9687e-01,  9.8792e-01, -9.9995e-01,  9.9996e-01,  9.3206e-01,\n",
      "         -5.3272e-01,  9.9888e-01, -9.9875e-01, -7.5171e-02,  8.5925e-01,\n",
      "         -2.4980e-01, -1.0000e+00, -9.6506e-01, -9.9991e-01,  2.2622e-01,\n",
      "         -6.9544e-02,  9.9776e-01, -8.6807e-01,  8.5181e-01, -9.8166e-01,\n",
      "         -9.9999e-01,  9.4052e-01, -5.2407e-01,  9.9965e-01, -9.8354e-01,\n",
      "          9.9969e-01,  9.9769e-01, -5.8381e-01, -9.9785e-01, -9.9999e-01,\n",
      "         -9.6259e-01,  1.0000e+00, -9.9847e-01, -6.2785e-01,  9.9998e-01,\n",
      "          9.8423e-01, -7.4102e-01, -9.8272e-01, -9.9914e-01, -9.9992e-01,\n",
      "         -4.9765e-01,  9.5752e-01, -9.0082e-01,  9.5920e-01, -9.9414e-01,\n",
      "         -6.5266e-01,  9.9971e-01,  1.0000e+00, -8.4595e-01,  9.6200e-01,\n",
      "          8.1853e-01, -9.8535e-01, -1.0000e+00, -9.9910e-01,  8.2280e-01,\n",
      "         -1.0000e+00,  9.9995e-01, -9.9371e-01,  1.0000e+00, -9.4994e-01,\n",
      "          9.5802e-01,  9.2406e-01, -7.4598e-01,  9.5026e-01,  7.6763e-01,\n",
      "          9.9999e-01,  9.1556e-01, -5.8729e-01,  6.9794e-01, -2.3932e-01,\n",
      "          7.9690e-01, -9.9467e-01,  7.7684e-01,  9.8639e-01,  4.8087e-01,\n",
      "         -9.3258e-01, -2.2234e-01, -9.6725e-01, -9.9102e-01, -7.8359e-01,\n",
      "          3.9922e-02,  5.9030e-01,  9.8097e-01,  7.6805e-01,  9.8591e-01,\n",
      "         -5.9295e-01,  9.6737e-01, -9.1836e-01, -1.0000e+00, -3.8100e-01,\n",
      "          8.1067e-01,  6.0243e-01,  9.8364e-01, -9.9974e-01,  9.9717e-01,\n",
      "         -9.8795e-01,  1.0000e+00, -5.4112e-01, -5.3221e-01,  8.1247e-01,\n",
      "          1.0000e+00, -9.9993e-01,  3.4165e-01, -9.5761e-01,  9.3197e-01,\n",
      "          9.9745e-01,  9.9958e-01, -7.9311e-01,  9.7233e-02,  4.1357e-01,\n",
      "          9.3965e-01,  6.0472e-01,  9.9049e-01,  1.0536e-01,  7.3538e-01,\n",
      "          9.1033e-01,  9.9939e-01,  9.9940e-01, -8.7577e-01, -8.5859e-01,\n",
      "         -9.9832e-01,  5.0362e-01,  5.6734e-01,  9.8036e-01,  7.4011e-01,\n",
      "         -7.2543e-01,  4.8458e-01, -7.4076e-01, -9.1068e-01,  9.6175e-01,\n",
      "         -3.6937e-01,  2.5731e-01, -9.8425e-01, -7.5519e-01, -5.0166e-01,\n",
      "          9.9990e-01, -9.9817e-01, -9.8165e-01,  9.9583e-01, -9.0488e-01,\n",
      "         -3.7974e-02,  2.1735e-01, -9.4153e-01, -9.1811e-01, -5.5850e-01,\n",
      "         -9.9993e-01,  8.1207e-01, -9.1077e-01,  5.1103e-01,  9.9927e-01,\n",
      "          4.8752e-01,  2.6731e-01, -8.7250e-01, -6.5548e-01,  9.1557e-01,\n",
      "          7.3502e-01,  8.5422e-01,  9.5933e-01, -6.4851e-01, -8.1060e-01,\n",
      "          9.7836e-01,  9.0498e-01, -8.8450e-01,  9.8112e-01, -9.5446e-01,\n",
      "          1.0000e+00, -5.0124e-01, -9.9999e-01,  6.0635e-01,  9.4697e-01,\n",
      "         -1.0000e+00, -9.9991e-01, -9.9999e-01,  9.9792e-01, -9.7602e-01,\n",
      "          9.6352e-01,  9.6432e-01, -9.9999e-01, -1.0000e+00, -7.5563e-02,\n",
      "         -9.9607e-01, -7.7761e-01, -9.9480e-01,  1.0000e+00,  7.0796e-01,\n",
      "          9.3007e-01, -3.4405e-01, -9.8866e-01,  9.9675e-01,  8.5127e-01,\n",
      "         -9.6038e-01, -9.9999e-01,  9.8616e-01, -5.2419e-01,  6.9961e-01,\n",
      "          9.9668e-01, -9.9712e-01, -7.2471e-01, -1.2969e-02,  9.3256e-01,\n",
      "          9.6171e-01,  6.2458e-01,  9.9956e-01, -1.0000e+00,  9.9758e-01,\n",
      "          9.8106e-01,  3.7018e-01,  7.1537e-01, -9.9130e-01,  9.9998e-01,\n",
      "         -9.8596e-01,  5.6852e-01, -7.7202e-01, -9.9998e-01, -8.5296e-01,\n",
      "          9.8721e-01,  9.7374e-01, -9.9618e-01,  6.8515e-01, -8.4746e-01,\n",
      "         -1.0000e+00,  8.9075e-01, -9.6142e-01,  9.9998e-01,  9.8934e-01,\n",
      "          9.3005e-01, -6.7102e-01, -7.9756e-01,  8.8797e-01, -1.0000e+00,\n",
      "         -7.8912e-01, -8.6222e-01, -8.5584e-01,  9.9945e-01,  9.8816e-01,\n",
      "          9.9927e-01,  9.5632e-01,  1.2389e-01, -9.9166e-01,  6.3469e-01,\n",
      "          4.8005e-01,  9.9976e-01, -8.9229e-01,  8.9603e-01, -7.0841e-01,\n",
      "         -9.9142e-01, -9.9027e-01,  9.9944e-01,  9.0652e-01,  9.2460e-01,\n",
      "         -1.4300e-01, -2.2865e-01,  9.6837e-01, -8.9932e-01,  4.0085e-01,\n",
      "         -1.0000e+00, -9.3977e-01, -7.5250e-01, -9.9998e-01,  9.9999e-01,\n",
      "         -1.0000e+00, -3.7370e-01,  9.9994e-01, -9.4392e-01,  9.9301e-01,\n",
      "          6.9363e-01, -1.0000e+00, -1.0000e+00, -8.8643e-01, -9.4355e-01,\n",
      "          9.9413e-01, -6.8888e-01,  2.7752e-01,  9.5229e-01,  9.9003e-01,\n",
      "          9.9961e-01,  9.9996e-01,  1.0000e+00,  8.6998e-01,  9.9995e-01,\n",
      "          4.0428e-01, -9.9985e-01, -5.1308e-01, -9.9998e-01,  3.6317e-01,\n",
      "          9.0335e-01,  9.9919e-01,  9.8105e-01,  8.8327e-01,  1.0000e+00,\n",
      "         -9.9999e-01,  9.9997e-01, -1.0000e+00,  9.3064e-01,  9.9997e-01,\n",
      "         -9.9671e-01,  5.7095e-01, -9.9995e-01,  8.7196e-01, -9.9170e-01,\n",
      "          8.7428e-01, -8.4508e-01,  9.9847e-01, -9.9998e-01, -9.9985e-01,\n",
      "         -9.2672e-01,  6.4223e-01,  9.9545e-01, -6.3388e-01,  9.8994e-01,\n",
      "          9.9964e-01,  1.9131e-01,  8.8567e-01, -8.3215e-01, -6.6961e-01,\n",
      "          1.0000e+00,  8.4386e-01,  9.9133e-01, -9.9625e-01,  9.8441e-01,\n",
      "          9.8117e-01,  7.0348e-01,  9.6511e-01, -3.4399e-01,  9.4644e-01,\n",
      "          9.1524e-01, -9.9967e-01,  4.2659e-01, -9.9999e-01,  6.0965e-01,\n",
      "         -8.0074e-01,  8.5450e-01,  8.3206e-01,  9.3277e-01,  7.4877e-01,\n",
      "         -9.9698e-01,  4.0668e-01, -9.9990e-01,  9.6714e-01,  9.2477e-01,\n",
      "          7.9382e-01, -8.2992e-01,  9.9420e-01,  4.3619e-01,  9.9987e-01,\n",
      "          9.9986e-01, -1.0000e+00,  8.5595e-01,  9.9900e-01,  9.7333e-01,\n",
      "          9.6473e-01, -9.9812e-01, -3.5901e-01,  8.6771e-01, -8.2082e-01,\n",
      "          9.9418e-01, -3.4919e-01, -6.5755e-01,  9.2536e-01, -9.9946e-01,\n",
      "          9.0897e-01, -6.0316e-01, -1.8271e-01,  8.3816e-01, -9.9288e-01,\n",
      "          5.0705e-01, -9.7950e-01, -3.5592e-01, -9.9996e-01, -9.8925e-01,\n",
      "         -9.9983e-01, -4.4201e-01,  9.9771e-01, -1.5439e-01,  9.9996e-01,\n",
      "         -1.3718e-01, -8.4047e-01, -6.5231e-01, -9.9996e-01,  8.7067e-01,\n",
      "          4.3143e-01, -7.9002e-01,  2.5700e-01, -9.3045e-01,  9.4168e-01,\n",
      "         -9.9980e-01, -1.0000e+00,  8.1477e-01, -4.9531e-01,  8.7681e-01,\n",
      "          9.6686e-01,  9.8964e-01, -7.1150e-01,  9.2309e-01, -5.5254e-01,\n",
      "          8.4219e-01, -7.2683e-01, -9.8892e-01,  9.1152e-01,  4.9038e-01,\n",
      "          1.0000e+00, -9.9918e-01, -9.6836e-01, -9.7300e-01, -9.8006e-01,\n",
      "         -9.4631e-01,  8.4793e-01, -4.1537e-01,  9.0754e-01,  2.1692e-01,\n",
      "          6.5177e-01,  9.9814e-01, -9.9927e-01, -9.9822e-01,  9.9997e-01,\n",
      "         -9.9988e-01,  9.5738e-01,  3.8204e-02, -5.6648e-01, -9.0085e-01,\n",
      "         -3.5596e-01,  1.3051e-01, -6.9099e-01, -8.3828e-01, -9.9999e-01,\n",
      "         -9.9875e-01,  9.9338e-01, -9.9593e-01, -9.1852e-01, -9.5132e-01,\n",
      "         -1.0000e+00,  9.9961e-01,  9.8364e-01,  1.0000e+00, -9.9996e-01,\n",
      "          7.2370e-01,  5.9476e-01,  9.9989e-01,  3.6717e-01, -8.2552e-01,\n",
      "         -9.6664e-01,  9.9995e-01,  9.9791e-01, -9.8921e-01,  7.2700e-01,\n",
      "         -8.2280e-01, -6.8965e-01, -8.7470e-01, -4.9336e-01,  9.4833e-01,\n",
      "          9.9533e-01, -9.9954e-01, -1.0000e+00,  9.9999e-01, -2.4578e-01,\n",
      "          9.9277e-01,  3.3741e-01, -8.7520e-01, -5.1480e-01, -6.5664e-01,\n",
      "         -5.8927e-01, -9.0685e-01, -1.0000e+00,  9.3255e-01, -1.0000e+00,\n",
      "         -9.9861e-01,  5.6284e-01,  9.9723e-01, -9.9961e-01, -9.8004e-01,\n",
      "          6.0156e-01, -1.0000e+00, -1.9843e-01, -1.0277e-01,  8.1548e-01,\n",
      "         -9.9250e-01, -9.2650e-01, -4.1621e-01,  9.9942e-01,  9.3478e-01,\n",
      "         -9.8715e-01, -9.8596e-01, -8.9456e-01,  1.0000e+00,  8.6000e-01,\n",
      "          8.4425e-01,  9.9015e-01,  7.0787e-01,  8.2186e-01, -8.6741e-01,\n",
      "         -9.9971e-01, -9.9789e-01, -2.9895e-04,  9.9898e-01,  9.9654e-01,\n",
      "         -9.9995e-01, -9.9967e-01, -2.0318e-01, -9.9313e-01,  9.9749e-01,\n",
      "         -8.9229e-01, -9.9998e-01, -9.9999e-01,  6.8146e-01, -8.6882e-01,\n",
      "          9.9734e-01, -5.0285e-01,  1.0000e+00,  8.3850e-01,  9.9399e-01,\n",
      "         -9.7288e-01,  9.6460e-01,  4.8054e-01,  9.8170e-01, -8.8926e-01,\n",
      "          1.0000e+00,  9.8795e-01,  9.9429e-01]], device='cuda:0')\n",
      "  Accuracy: 1.000\n",
      "  Test Loss: 0.011\n",
      "  Test took: 0:00:00\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    10  of     11.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss generetor: 0.717\n",
      "  Average training loss discriminator: 0.741\n",
      "  Training epcoh took: 0:00:04\n",
      "\n",
      "Running Test...\n",
      "Evaluation tensor([[-0.9504,  0.8993,  1.0000, -0.9983,  0.9063, -0.9394,  0.9692,  0.5988,\n",
      "         -0.9274,  0.3649,  0.9929,  0.9991,  0.9026, -0.9998, -0.9955, -0.9705,\n",
      "          0.9849, -0.9723, -1.0000,  0.9850,  0.5998, -1.0000,  0.3268, -0.6122,\n",
      "          0.9241,  0.5913,  0.9937,  1.0000,  0.7081,  0.9993,  0.6597, -0.9907,\n",
      "         -0.9703, -0.9999,  0.7616, -0.4935, -0.9938, -0.8376, -0.9693,  0.6636,\n",
      "         -0.8529,  0.9754, -0.9773, -0.7782, -0.9473,  0.7381,  0.9165,  0.1599,\n",
      "         -0.6860,  1.0000, -0.9834,  1.0000,  0.3742,  1.0000,  0.9963,  0.8319,\n",
      "          0.9954,  0.7901,  0.7759,  0.9717,  0.7636, -0.6454,  0.9855, -0.6268,\n",
      "         -0.9988, -0.9304,  0.9891,  0.7784, -0.6556,  0.8070,  0.6733,  0.6943,\n",
      "          0.9986, -0.9964, -0.7837, -0.9887,  0.9969, -1.0000,  0.9911,  1.0000,\n",
      "         -0.9858, -0.9999,  0.9979, -0.8681,  0.9931, -0.9999,  0.8161, -1.0000,\n",
      "          0.6284, -0.6198, -0.3852, -0.9974, -0.3584,  0.9529,  1.0000,  0.8938,\n",
      "         -0.8860,  0.8530, -0.7357,  0.8946, -0.7763, -0.9717, -0.5164,  0.7448,\n",
      "          0.5346, -0.9992, -0.6922,  0.8398, -1.0000,  0.8235,  0.9778, -0.9629,\n",
      "          0.9868,  0.8893, -0.9347,  0.1331,  0.9944, -0.8989, -0.9244,  1.0000,\n",
      "          0.3001, -0.7796,  0.9997, -0.9925,  0.8367, -0.6598,  0.9934, -0.9243,\n",
      "          0.9972, -0.6743,  0.9365, -0.9958,  0.6311,  0.9999, -0.7463,  1.0000,\n",
      "         -0.9996, -0.7118, -1.0000,  0.3387,  0.3444,  0.3892,  0.7809,  0.9902,\n",
      "          0.9846,  0.7336,  0.8590,  0.6598, -0.9302,  0.8349,  0.9909,  0.9935,\n",
      "         -0.9263,  1.0000, -0.8470, -0.6065, -0.9385,  0.8570,  0.9334,  0.1826,\n",
      "          0.9943, -0.9998, -0.9981, -0.2050,  1.0000,  0.9783, -0.9558,  0.9504,\n",
      "          1.0000,  0.9998, -0.0345, -0.9967, -0.9603,  0.8710,  0.5419,  0.9314,\n",
      "          0.8920,  1.0000, -0.9947,  1.0000,  1.0000,  0.8497,  0.8793, -0.5475,\n",
      "         -0.9994, -0.9970, -0.9966,  0.9681, -0.9988, -0.9968, -0.6254,  0.9870,\n",
      "         -0.9435,  0.6515, -1.0000, -0.1792,  0.9951, -0.9249,  1.0000,  0.9993,\n",
      "         -1.0000,  0.8366, -0.9389,  0.5451, -0.8552,  0.7249,  0.4218,  0.8200,\n",
      "          0.7103, -1.0000, -0.9522, -0.4982, -0.9916,  0.2249,  0.9942,  0.9822,\n",
      "         -0.7641,  0.6752,  0.9475,  1.0000, -0.9999,  0.4181,  0.8129, -0.9997,\n",
      "         -0.9983,  0.9939, -0.5686,  0.9993, -0.8083, -1.0000,  0.7661, -0.9466,\n",
      "          0.9981,  0.9768,  0.9994, -1.0000,  0.8784, -0.7864, -0.2515,  0.6966,\n",
      "          0.4397, -0.9230, -0.8383,  0.8877,  0.9504,  0.5291,  0.3118, -0.9746,\n",
      "          0.9974,  0.6064, -0.9973,  0.9877, -0.9999,  1.0000,  0.9287, -0.4986,\n",
      "          0.9988, -0.9986, -0.0465,  0.8472, -0.2665, -1.0000, -0.9689, -0.9999,\n",
      "          0.1315, -0.0483,  0.9976, -0.8477,  0.8314, -0.9811, -1.0000,  0.9404,\n",
      "         -0.5042,  0.9996, -0.9825,  0.9997,  0.9976, -0.5792, -0.9978, -1.0000,\n",
      "         -0.9635,  1.0000, -0.9983, -0.6417,  1.0000,  0.9820, -0.7388, -0.9823,\n",
      "         -0.9990, -0.9999, -0.5255,  0.9544, -0.9024,  0.9529, -0.9949, -0.6733,\n",
      "          0.9997,  1.0000, -0.8548,  0.9666,  0.8270, -0.9855, -1.0000, -0.9992,\n",
      "          0.8299, -1.0000,  0.9999, -0.9933,  1.0000, -0.9412,  0.9565,  0.9113,\n",
      "         -0.7685,  0.9473,  0.7925,  1.0000,  0.8921, -0.5910,  0.7008, -0.2389,\n",
      "          0.7994, -0.9948,  0.7470,  0.9871,  0.4663, -0.9300, -0.1585, -0.9644,\n",
      "         -0.9900, -0.7668,  0.0475,  0.5008,  0.9820,  0.7225,  0.9838, -0.6131,\n",
      "          0.9676, -0.9160, -1.0000, -0.4783,  0.8182,  0.5816,  0.9856, -0.9998,\n",
      "          0.9969, -0.9848,  1.0000, -0.5799, -0.5498,  0.8086,  1.0000, -0.9999,\n",
      "          0.3247, -0.9619,  0.9301,  0.9975,  0.9996, -0.7816,  0.1393,  0.3553,\n",
      "          0.9429,  0.5387,  0.9898,  0.0156,  0.7411,  0.9076,  0.9994,  0.9994,\n",
      "         -0.8728, -0.8632, -0.9985,  0.4519,  0.4271,  0.9794,  0.7306, -0.7382,\n",
      "          0.5385, -0.7448, -0.9116,  0.9603, -0.4181,  0.2693, -0.9810, -0.7451,\n",
      "         -0.5230,  0.9999, -0.9981, -0.9843,  0.9952, -0.9067,  0.0141,  0.1434,\n",
      "         -0.9332, -0.9060, -0.5861, -0.9999,  0.8256, -0.9104,  0.5328,  0.9993,\n",
      "          0.4880,  0.2485, -0.8673, -0.6722,  0.9135,  0.7477,  0.8355,  0.9601,\n",
      "         -0.6381, -0.8169,  0.9791,  0.9152, -0.8855,  0.9770, -0.9458,  1.0000,\n",
      "         -0.5293, -1.0000,  0.5413,  0.9508, -1.0000, -0.9999, -1.0000,  0.9977,\n",
      "         -0.9741,  0.9599,  0.9632, -1.0000, -1.0000, -0.0093, -0.9959, -0.7833,\n",
      "         -0.9960,  1.0000,  0.7322,  0.9362, -0.3289, -0.9873,  0.9974,  0.8306,\n",
      "         -0.9597, -1.0000,  0.9870, -0.5434,  0.7055,  0.9968, -0.9971, -0.7922,\n",
      "          0.1180,  0.9305,  0.9519,  0.6141,  0.9996, -1.0000,  0.9974,  0.9812,\n",
      "          0.3587,  0.7402, -0.9896,  1.0000, -0.9843,  0.5892, -0.7894, -1.0000,\n",
      "         -0.8443,  0.9856,  0.9748, -0.9957,  0.7129, -0.8078, -1.0000,  0.8923,\n",
      "         -0.9578,  1.0000,  0.9878,  0.9316, -0.6968, -0.7616,  0.8859, -1.0000,\n",
      "         -0.7737, -0.8593, -0.8070,  0.9995,  0.9861,  0.9992,  0.9546,  0.1011,\n",
      "         -0.9902,  0.6539,  0.3575,  0.9998, -0.8914,  0.9065, -0.7169, -0.9896,\n",
      "         -0.9908,  0.9994,  0.9002,  0.9065, -0.1076, -0.1459,  0.9711, -0.9036,\n",
      "          0.3516, -1.0000, -0.9382, -0.7718, -1.0000,  1.0000, -1.0000, -0.4023,\n",
      "          1.0000, -0.9425,  0.9918,  0.6895, -1.0000, -1.0000, -0.8669, -0.9466,\n",
      "          0.9931, -0.7072,  0.2043,  0.9526,  0.9914,  0.9996,  1.0000,  1.0000,\n",
      "          0.8473,  0.9999,  0.4230, -0.9999, -0.5096, -1.0000,  0.4007,  0.8950,\n",
      "          0.9992,  0.9802,  0.8694,  1.0000, -1.0000,  1.0000, -1.0000,  0.9294,\n",
      "          1.0000, -0.9962,  0.5578, -0.9999,  0.8559, -0.9911,  0.8845, -0.8448,\n",
      "          0.9984, -1.0000, -0.9999, -0.9368,  0.5778,  0.9953, -0.6312,  0.9905,\n",
      "          0.9996,  0.1864,  0.8609, -0.8451, -0.6396,  1.0000,  0.8345,  0.9904,\n",
      "         -0.9959,  0.9823,  0.9815,  0.7023,  0.9637, -0.3540,  0.9536,  0.9218,\n",
      "         -0.9996,  0.4504, -1.0000,  0.6889, -0.7902,  0.8572,  0.8539,  0.9357,\n",
      "          0.7599, -0.9965,  0.4259, -0.9999,  0.9625,  0.9116,  0.8004, -0.8440,\n",
      "          0.9942,  0.5577,  0.9999,  0.9998, -1.0000,  0.8707,  0.9989,  0.9732,\n",
      "          0.9574, -0.9981, -0.3523,  0.8872, -0.8325,  0.9939, -0.4204, -0.6519,\n",
      "          0.9206, -0.9994,  0.9080, -0.6074, -0.2057,  0.8247, -0.9920,  0.5219,\n",
      "         -0.9770, -0.3993, -1.0000, -0.9873, -0.9998, -0.4575,  0.9975, -0.2311,\n",
      "          1.0000, -0.3159, -0.8396, -0.6683, -1.0000,  0.8588,  0.4158, -0.7951,\n",
      "          0.2459, -0.9219,  0.9433, -0.9998, -1.0000,  0.8222, -0.4168,  0.8794,\n",
      "          0.9658,  0.9890, -0.6682,  0.9163, -0.5428,  0.8478, -0.7100, -0.9862,\n",
      "          0.9023,  0.5571,  1.0000, -0.9992, -0.9701, -0.9699, -0.9808, -0.9479,\n",
      "          0.8527, -0.4324,  0.9027,  0.2374,  0.6597,  0.9981, -0.9993, -0.9980,\n",
      "          1.0000, -0.9999,  0.9532,  0.0486, -0.5678, -0.9046, -0.3753,  0.1946,\n",
      "         -0.6260, -0.8419, -1.0000, -0.9989,  0.9930, -0.9949, -0.9154, -0.9548,\n",
      "         -1.0000,  0.9996,  0.9838,  1.0000, -1.0000,  0.6964,  0.5846,  0.9999,\n",
      "          0.4179, -0.8291, -0.9646,  1.0000,  0.9980, -0.9896,  0.7641, -0.8307,\n",
      "         -0.7230, -0.8688, -0.4440,  0.9430,  0.9954, -0.9995, -1.0000,  1.0000,\n",
      "         -0.2320,  0.9921,  0.3576, -0.8723, -0.4399, -0.6560, -0.6342, -0.8977,\n",
      "         -1.0000,  0.9371, -1.0000, -0.9985,  0.5789,  0.9964, -0.9996, -0.9767,\n",
      "          0.6136, -1.0000, -0.0680, -0.1767,  0.8040, -0.9909, -0.9265, -0.4345,\n",
      "          0.9994,  0.9259, -0.9853, -0.9863, -0.8962,  1.0000,  0.8763,  0.8492,\n",
      "          0.9909,  0.6967,  0.7988, -0.8353, -0.9997, -0.9978,  0.1332,  0.9989,\n",
      "          0.9962, -0.9999, -0.9997, -0.0844, -0.9937,  0.9973, -0.8910, -1.0000,\n",
      "         -1.0000,  0.6853, -0.8789,  0.9970, -0.5568,  1.0000,  0.8358,  0.9942,\n",
      "         -0.9722,  0.9658,  0.4960,  0.9848, -0.8991,  1.0000,  0.9889,  0.9939]],\n",
      "       device='cuda:0')\n",
      "  Accuracy: 1.000\n",
      "  Test Loss: 0.009\n",
      "  Test took: 0:00:00\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "  Batch    10  of     11.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss generetor: 0.714\n",
      "  Average training loss discriminator: 0.748\n",
      "  Training epcoh took: 0:00:04\n",
      "\n",
      "Running Test...\n",
      "Evaluation tensor([[-0.9454,  0.9124,  1.0000, -0.9979,  0.8987, -0.9427,  0.9378,  0.7507,\n",
      "         -0.8893,  0.4774,  0.9879,  0.9988,  0.9473, -0.9997, -0.9969, -0.9608,\n",
      "          0.9761, -0.9724, -1.0000,  0.9901,  0.7027, -1.0000,  0.3534, -0.7482,\n",
      "          0.8857,  0.6133,  0.9908,  1.0000,  0.5202,  0.9996,  0.6408, -0.9837,\n",
      "         -0.9736, -0.9999,  0.7993, -0.5255, -0.9957, -0.8486, -0.9819,  0.7334,\n",
      "         -0.8441,  0.9849, -0.9821, -0.7853, -0.9632,  0.6965,  0.9340,  0.2174,\n",
      "         -0.7286,  1.0000, -0.9751,  1.0000,  0.5183,  1.0000,  0.9951,  0.8234,\n",
      "          0.9917,  0.7979,  0.8905,  0.9770,  0.6335, -0.6470,  0.9830, -0.6899,\n",
      "         -0.9994, -0.9324,  0.9944,  0.7751, -0.6801,  0.7578,  0.6761,  0.7307,\n",
      "          0.9982, -0.9957, -0.8163, -0.9836,  0.9983, -1.0000,  0.9892,  1.0000,\n",
      "         -0.9906, -0.9999,  0.9969, -0.8852,  0.9964, -0.9999,  0.8959, -0.9999,\n",
      "          0.5987, -0.5363, -0.5131, -0.9968, -0.4774,  0.9718,  1.0000,  0.9218,\n",
      "         -0.8905,  0.8617, -0.7590,  0.9247, -0.7188, -0.9844, -0.6658,  0.8180,\n",
      "          0.4153, -0.9996, -0.4770,  0.8744, -1.0000,  0.8469,  0.9620, -0.9372,\n",
      "          0.9917,  0.9079, -0.9575,  0.3726,  0.9911, -0.9213, -0.9277,  1.0000,\n",
      "          0.2767, -0.8044,  0.9996, -0.9935,  0.8676, -0.6366,  0.9964, -0.9334,\n",
      "          0.9984, -0.7375,  0.9345, -0.9936,  0.7760,  0.9999, -0.7618,  1.0000,\n",
      "         -0.9994, -0.7846, -1.0000,  0.4311,  0.2743,  0.4438,  0.8396,  0.9946,\n",
      "          0.9722,  0.7688,  0.8899,  0.7384, -0.9441,  0.8286,  0.9910,  0.9922,\n",
      "         -0.8848,  1.0000, -0.9212, -0.6852, -0.9433,  0.8591,  0.9471, -0.0035,\n",
      "          0.9919, -0.9997, -0.9986, -0.0729,  1.0000,  0.9685, -0.9712,  0.9724,\n",
      "          0.9999,  0.9999,  0.0069, -0.9981, -0.9648,  0.9122,  0.5250,  0.9327,\n",
      "          0.8952,  1.0000, -0.9918,  1.0000,  1.0000,  0.8557,  0.8810, -0.7454,\n",
      "         -0.9993, -0.9954, -0.9953,  0.9777, -0.9994, -0.9979, -0.6898,  0.9845,\n",
      "         -0.9672,  0.6270, -1.0000, -0.1470,  0.9941, -0.9399,  1.0000,  0.9997,\n",
      "         -1.0000,  0.8661, -0.9540,  0.3821, -0.8724,  0.5083,  0.5202,  0.8683,\n",
      "          0.6231, -1.0000, -0.9711, -0.6138, -0.9949,  0.0247,  0.9915,  0.9896,\n",
      "         -0.7845,  0.7148,  0.9608,  1.0000, -0.9999,  0.4567,  0.8190, -0.9997,\n",
      "         -0.9976,  0.9902, -0.6006,  0.9994, -0.7907, -1.0000,  0.7778, -0.9557,\n",
      "          0.9975,  0.9829,  0.9997, -1.0000,  0.9251, -0.7296, -0.1324,  0.7289,\n",
      "          0.3815, -0.9371, -0.7696,  0.9357,  0.9302,  0.3697,  0.4097, -0.9806,\n",
      "          0.9984,  0.7629, -0.9983,  0.9918, -0.9999,  0.9999,  0.9545, -0.6610,\n",
      "          0.9989, -0.9982, -0.0763,  0.9013, -0.2889, -1.0000, -0.9767, -1.0000,\n",
      "          0.1546, -0.0025,  0.9969, -0.6964,  0.8654, -0.9875, -1.0000,  0.9424,\n",
      "         -0.4682,  0.9994, -0.9854,  0.9999,  0.9971, -0.6699, -0.9971, -1.0000,\n",
      "         -0.9774,  1.0000, -0.9974, -0.6519,  1.0000,  0.9899, -0.7440, -0.9791,\n",
      "         -0.9985, -0.9999, -0.6548,  0.9717, -0.9050,  0.9320, -0.9970, -0.7330,\n",
      "          0.9997,  1.0000, -0.8749,  0.9810,  0.8438, -0.9795, -1.0000, -0.9997,\n",
      "          0.8541, -1.0000,  0.9999, -0.9905,  1.0000, -0.9467,  0.9767,  0.8681,\n",
      "         -0.7850,  0.9623,  0.8355,  1.0000,  0.7853, -0.6200,  0.6893, -0.2950,\n",
      "          0.8321, -0.9964,  0.7916,  0.9923,  0.4564, -0.9075, -0.3391, -0.9739,\n",
      "         -0.9842, -0.8485,  0.1201,  0.3377,  0.9886,  0.5439,  0.9699, -0.6519,\n",
      "          0.9800, -0.9222, -1.0000, -0.3937,  0.8560,  0.7426,  0.9908, -0.9999,\n",
      "          0.9960, -0.9737,  1.0000, -0.6322, -0.6298,  0.8888,  1.0000, -0.9999,\n",
      "          0.2751, -0.9758,  0.9429,  0.9985,  0.9993, -0.8444, -0.0774,  0.2891,\n",
      "          0.9593,  0.3636,  0.9880,  0.0699,  0.7637,  0.9503,  0.9996,  0.9991,\n",
      "         -0.9307, -0.8773, -0.9991,  0.6492,  0.0281,  0.9825,  0.8215, -0.7844,\n",
      "          0.6675, -0.8317, -0.9517,  0.9764, -0.5540,  0.3142, -0.9679, -0.8397,\n",
      "         -0.6069,  0.9999, -0.9975, -0.9879,  0.9927, -0.9083,  0.2256,  0.0109,\n",
      "         -0.9607, -0.8763, -0.5929, -0.9999,  0.8349, -0.9040,  0.5913,  0.9997,\n",
      "          0.4625,  0.4054, -0.9057, -0.7435,  0.8977,  0.7968,  0.7024,  0.9630,\n",
      "         -0.6616, -0.8441,  0.9838,  0.9321, -0.9141,  0.9663, -0.9086,  1.0000,\n",
      "         -0.5316, -1.0000,  0.6853,  0.9780, -1.0000, -1.0000, -1.0000,  0.9966,\n",
      "         -0.9820,  0.9771,  0.9813, -1.0000, -1.0000, -0.0526, -0.9972, -0.8151,\n",
      "         -0.9982,  1.0000,  0.7707,  0.9502, -0.2754, -0.9814,  0.9984,  0.8969,\n",
      "         -0.9794, -1.0000,  0.9915, -0.7309,  0.8251,  0.9981, -0.9971, -0.8109,\n",
      "          0.3308,  0.9332,  0.9149,  0.6905,  0.9998, -1.0000,  0.9966,  0.9868,\n",
      "          0.3789,  0.7929, -0.9841,  1.0000, -0.9886,  0.6319, -0.8111, -1.0000,\n",
      "         -0.8869,  0.9898,  0.9835, -0.9939,  0.7434, -0.6522, -1.0000,  0.9100,\n",
      "         -0.9722,  1.0000,  0.9802,  0.9422, -0.6998, -0.6448,  0.8843, -1.0000,\n",
      "         -0.8113, -0.8894, -0.6192,  0.9997,  0.9777,  0.9989,  0.9638,  0.1661,\n",
      "         -0.9938,  0.6696, -0.0567,  0.9999, -0.8988,  0.9155, -0.7205, -0.9806,\n",
      "         -0.9885,  0.9993,  0.9479,  0.8116, -0.2991, -0.3580,  0.9816, -0.9093,\n",
      "          0.4683, -1.0000, -0.9317, -0.8282, -1.0000,  1.0000, -1.0000, -0.5084,\n",
      "          1.0000, -0.9608,  0.9879,  0.6965, -1.0000, -1.0000, -0.9001, -0.9617,\n",
      "          0.9876, -0.7581,  0.1159,  0.9728,  0.9920,  0.9994,  1.0000,  1.0000,\n",
      "          0.9025,  0.9999,  0.4001, -0.9998, -0.7192, -1.0000,  0.2926,  0.8597,\n",
      "          0.9992,  0.9725,  0.9277,  1.0000, -1.0000,  0.9999, -1.0000,  0.9624,\n",
      "          1.0000, -0.9934,  0.6380, -0.9999,  0.9082, -0.9943,  0.9048, -0.8481,\n",
      "          0.9979, -1.0000, -0.9998, -0.9412,  0.5980,  0.9965, -0.7768,  0.9953,\n",
      "          0.9996,  0.2759,  0.7630, -0.8723, -0.7881,  1.0000,  0.8892,  0.9909,\n",
      "         -0.9943,  0.9674,  0.9882,  0.6985,  0.9487, -0.3865,  0.9767,  0.9305,\n",
      "         -0.9995,  0.4173, -1.0000,  0.7203, -0.8243,  0.8528,  0.8808,  0.9565,\n",
      "          0.7918, -0.9943,  0.4218, -0.9998,  0.9313,  0.9279,  0.8240, -0.8647,\n",
      "          0.9964,  0.7395,  0.9998,  0.9998, -1.0000,  0.8993,  0.9984,  0.9840,\n",
      "          0.9285, -0.9973, -0.3839,  0.8867, -0.8201,  0.9912, -0.4838, -0.6545,\n",
      "          0.8881, -0.9992,  0.9377, -0.5726, -0.3247,  0.8015, -0.9899,  0.5775,\n",
      "         -0.9852, -0.3120, -0.9999, -0.9889, -0.9997, -0.5072,  0.9968, -0.0152,\n",
      "          0.9999, -0.3355, -0.8582, -0.6931, -0.9999,  0.9208,  0.4011, -0.8153,\n",
      "          0.2746, -0.9554,  0.9489, -0.9999, -1.0000,  0.8500, -0.5247,  0.8764,\n",
      "          0.9578,  0.9940, -0.4502,  0.9310, -0.4054,  0.8645, -0.7584, -0.9828,\n",
      "          0.9008,  0.7271,  1.0000, -0.9991, -0.9631, -0.9464, -0.9886, -0.9632,\n",
      "          0.8566, -0.4895,  0.9390,  0.1021,  0.7229,  0.9974, -0.9991, -0.9968,\n",
      "          0.9999, -0.9999,  0.9715, -0.0146, -0.5944, -0.9234, -0.4580,  0.1658,\n",
      "         -0.5022, -0.8514, -1.0000, -0.9993,  0.9955, -0.9917, -0.9069, -0.9656,\n",
      "         -1.0000,  0.9994,  0.9813,  1.0000, -0.9999,  0.6442,  0.5713,  0.9998,\n",
      "          0.4728, -0.8484, -0.9734,  0.9999,  0.9990, -0.9950,  0.8092, -0.8461,\n",
      "         -0.7936, -0.8968, -0.5646,  0.9453,  0.9969, -0.9993, -1.0000,  1.0000,\n",
      "         -0.2659,  0.9881,  0.3916, -0.8760, -0.2800, -0.7582, -0.5619, -0.8286,\n",
      "         -1.0000,  0.9494, -1.0000, -0.9981,  0.6072,  0.9940, -0.9994, -0.9592,\n",
      "          0.6692, -1.0000, -0.0297, -0.1097,  0.8462, -0.9845, -0.9565, -0.4459,\n",
      "          0.9997,  0.8966, -0.9800, -0.9920, -0.9183,  1.0000,  0.9003,  0.8806,\n",
      "          0.9951,  0.8008,  0.8125, -0.7389, -0.9998, -0.9974,  0.4389,  0.9984,\n",
      "          0.9941, -0.9999, -0.9995, -0.2420, -0.9968,  0.9965, -0.8890, -1.0000,\n",
      "         -1.0000,  0.7006, -0.9052,  0.9954, -0.6124,  1.0000,  0.7772,  0.9964,\n",
      "         -0.9835,  0.9733,  0.5277,  0.9898, -0.9133,  1.0000,  0.9935,  0.9925]],\n",
      "       device='cuda:0')\n",
      "  Accuracy: 1.000\n",
      "  Test Loss: 0.007\n",
      "  Test took: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------\n",
    "#   Transfer Objects to GPU\n",
    "#-------------------------------------------------\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "#models parameters\n",
    "transformer_vars = [i for i in transformer.parameters()]\n",
    "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
    "g_vars = [v for v in generator.parameters()]\n",
    "\n",
    "#optimizer\n",
    "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
    "gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) \n",
    "\n",
    "#scheduler\n",
    "if apply_scheduler:\n",
    "    num_train_examples = len(train_examples)\n",
    "    num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
    "    num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
    "\n",
    "    scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)\n",
    "    scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)\n",
    "# num_train_epochs =1 \n",
    "# For each epoch...\n",
    "for epoch_i in range(0, num_train_epochs):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_train_epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    tr_g_loss = 0\n",
    "    tr_d_loss = 0\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    transformer.train() \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every print_each_n_step batches.\n",
    "        if step % print_each_n_step == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_label_mask = batch[3].to(device)\n",
    "\n",
    "        real_batch_size = b_input_ids.shape[0]\n",
    "     \n",
    "        # Encode real data in the Transformer\n",
    "        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
    "        hidden_states = model_outputs[-1]\n",
    "        \n",
    "        # Generate fake data that should have the same distribution of the ones\n",
    "        # encoded by the transformer. \n",
    "        # First noisy input are used in input to the Generator\n",
    "        noise = torch.zeros(real_batch_size, noise_size, device=device).uniform_(0, 1)\n",
    "        # Gnerate Fake data\n",
    "        gen_rep = generator(noise)\n",
    "\n",
    "        # Generate the output of the Discriminator for real and fake data.\n",
    "        # First, we put together the output of the tranformer and the generator\n",
    "        disciminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n",
    "        # Then, we select the output of the disciminator\n",
    "        features, logits, probs = discriminator(disciminator_input)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Finally, we separate the discriminator's output for the real and fake\n",
    "        # data\n",
    "        features_list = torch.split(features, real_batch_size)\n",
    "        D_real_features = features_list[0]\n",
    "        D_fake_features = features_list[1]\n",
    "      \n",
    "        logits_list = torch.split(logits, real_batch_size)\n",
    "        D_real_logits = logits_list[0]\n",
    "        D_fake_logits = logits_list[1]\n",
    "        \n",
    "        probs_list = torch.split(probs, real_batch_size)\n",
    "        D_real_probs = probs_list[0]\n",
    "        D_fake_probs = probs_list[1]\n",
    "\n",
    "        #---------------------------------\n",
    "        #  LOSS evaluation\n",
    "        #---------------------------------\n",
    "        # Generator's LOSS estimation\n",
    "        g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n",
    "        g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
    "        g_loss = g_loss_d + g_feat_reg\n",
    "  \n",
    "        # Disciminator's LOSS estimation\n",
    "        logits = D_real_logits[:,0:-1]\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        # The discriminator provides an output for labeled and unlabeled real data\n",
    "        # so the loss evaluated for unlabeled data is ignored (masked)\n",
    "        label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_list))\n",
    "        per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
    "        per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
    "        labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
    "\n",
    "        # It may be the case that a batch does not contain labeled examples, \n",
    "        # so the \"supervised loss\" in this case is not evaluated\n",
    "        if labeled_example_count == 0:\n",
    "            D_L_Supervised = 0\n",
    "        else:\n",
    "            D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
    "                 \n",
    "        D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
    "        D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
    "        d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
    "\n",
    "        #---------------------------------\n",
    "        #  OPTIMIZATION\n",
    "        #---------------------------------\n",
    "        # Avoid gradient accumulation\n",
    "        gen_optimizer.zero_grad()\n",
    "        dis_optimizer.zero_grad()\n",
    "\n",
    "        # Calculate weigth updates\n",
    "        # retain_graph=True is required since the underlying graph will be deleted after backward\n",
    "        g_loss.backward(retain_graph=True)\n",
    "        d_loss.backward() \n",
    "        \n",
    "        # Apply modifications\n",
    "        gen_optimizer.step()\n",
    "        dis_optimizer.step()\n",
    "\n",
    "        # A detail log of the individual losses\n",
    "        #print(\"{0:.4f}\\t{1:.4f}\\t{2:.4f}\\t{3:.4f}\\t{4:.4f}\".\n",
    "        #      format(D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n",
    "        #             g_loss_d, g_feat_reg))\n",
    "\n",
    "        # Save the losses to print them later\n",
    "        tr_g_loss += g_loss.item()\n",
    "        tr_d_loss += d_loss.item()\n",
    "\n",
    "        # Update the learning rate with the scheduler\n",
    "        if apply_scheduler:\n",
    "            scheduler_d.step()\n",
    "            scheduler_g.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
    "    avg_train_loss_d = tr_d_loss / len(train_dataloader)             \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss generetor: {0:.3f}\".format(avg_train_loss_g))\n",
    "    print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #     TEST ON THE EVALUATION DATASET\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our test set.\n",
    "    print(\"\")\n",
    "    print(\"Running Test...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    transformer.eval() #maybe redundant\n",
    "    discriminator.eval()\n",
    "    generator.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_test_accuracy = 0\n",
    "   \n",
    "    total_test_loss = 0\n",
    "    nb_test_steps = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels_ids = []\n",
    "\n",
    "    #loss\n",
    "    nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in test_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
    "            hidden_states = model_outputs[-1]\n",
    "            _, logits, probs = discriminator(hidden_states)\n",
    "            print(\"Evaluation\",hidden_states)\n",
    "            ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
    "            filtered_logits = logits[:,0:-1]\n",
    "            # Accumulate the test loss.\n",
    "            total_test_loss += nll_loss(filtered_logits, b_labels)\n",
    "            \n",
    "        # Accumulate the predictions and the input labels\n",
    "        _, preds = torch.max(filtered_logits, 1)\n",
    "        all_preds += preds.detach().cpu()\n",
    "        all_labels_ids += b_labels.detach().cpu()\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    all_preds = torch.stack(all_preds).numpy()\n",
    "    all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
    "    test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
    "    print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "    avg_test_loss = avg_test_loss.item()\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    test_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
    "    print(\"  Test took: {:}\".format(test_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss generator': avg_train_loss_g,\n",
    "            'Training Loss discriminator': avg_train_loss_d,\n",
    "            'Valid. Loss': avg_test_loss,\n",
    "            'Valid. Accur.': test_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Test Time': test_time\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd9efa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'Training Loss generator': 0.45681632377884607, 'Training Loss discriminator': 2.159539677880027, 'Valid. Loss': 0.5004576444625854, 'Valid. Accur.': 1.0, 'Training Time': '0:00:04', 'Test Time': '0:00:00'}\n",
      "{'epoch': 2, 'Training Loss generator': 0.6681772470474243, 'Training Loss discriminator': 1.3571972521868618, 'Valid. Loss': 0.20002782344818115, 'Valid. Accur.': 1.0, 'Training Time': '0:00:04', 'Test Time': '0:00:00'}\n",
      "{'epoch': 3, 'Training Loss generator': 0.8358810977502302, 'Training Loss discriminator': 0.818211089481007, 'Valid. Loss': 0.036212772130966187, 'Valid. Accur.': 1.0, 'Training Time': '0:00:04', 'Test Time': '0:00:00'}\n",
      "{'epoch': 4, 'Training Loss generator': 0.7768805243752219, 'Training Loss discriminator': 0.8545701612125743, 'Valid. Loss': 0.014373105950653553, 'Valid. Accur.': 1.0, 'Training Time': '0:00:04', 'Test Time': '0:00:00'}\n",
      "{'epoch': 5, 'Training Loss generator': 0.7217735268852927, 'Training Loss discriminator': 0.8918275508013639, 'Valid. Loss': 0.014414937235414982, 'Valid. Accur.': 1.0, 'Training Time': '0:00:04', 'Test Time': '0:00:00'}\n",
      "{'epoch': 6, 'Training Loss generator': 0.7426200725815513, 'Training Loss discriminator': 0.7988157218152826, 'Valid. Loss': 0.039844151586294174, 'Valid. Accur.': 1.0, 'Training Time': '0:00:04', 'Test Time': '0:00:00'}\n",
      "{'epoch': 7, 'Training Loss generator': 0.7315420183268461, 'Training Loss discriminator': 0.7916108749129556, 'Valid. Loss': 0.014454183168709278, 'Valid. Accur.': 1.0, 'Training Time': '0:00:04', 'Test Time': '0:00:00'}\n",
      "{'epoch': 8, 'Training Loss generator': 0.7216149135069414, 'Training Loss discriminator': 0.7758736176924272, 'Valid. Loss': 0.010986197739839554, 'Valid. Accur.': 1.0, 'Training Time': '0:00:04', 'Test Time': '0:00:00'}\n",
      "{'epoch': 9, 'Training Loss generator': 0.7166452299464833, 'Training Loss discriminator': 0.7408325888893821, 'Valid. Loss': 0.008725256659090519, 'Valid. Accur.': 1.0, 'Training Time': '0:00:04', 'Test Time': '0:00:00'}\n",
      "{'epoch': 10, 'Training Loss generator': 0.7138795473358848, 'Training Loss discriminator': 0.747500869360837, 'Valid. Loss': 0.006902776658535004, 'Valid. Accur.': 1.0, 'Training Time': '0:00:04', 'Test Time': '0:00:00'}\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:00:36 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "for stat in training_stats:\n",
    "    print(stat)\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d6a852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "                {\n",
    "                    'tokenizer': tokenizer,\n",
    "                    'bert_encoder': transformer.state_dict(),\n",
    "                    'discriminator': discriminator.state_dict()\n",
    "            }, \n",
    "    f\"gan_bert_finetuned_sst2_{len(train_examples)}_samples_{datetime.now():%Y-%m-%d_%H-%M-%S%z}.pt\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3b7701",
   "metadata": {},
   "source": [
    "# Rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b12eaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f80d361",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train_dataloader.dataset.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e23f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b82b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e40301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp_list = []\n",
    "\n",
    "tmp_list.append(AutoModel.from_pretrained(model_name))\n",
    "tmp_list.append(Discriminator(input_size=hidden_size, \n",
    "                              hidden_sizes=hidden_levels_d, \n",
    "                              num_labels=len(label_list), \n",
    "                              dropout_rate=out_dropout_rate\n",
    "                             ).cuda(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a222d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributed.pipeline.sync import Pipe\n",
    "import tempfile\n",
    "\n",
    "from torch.distributed import rpc\n",
    "tmpfile = tempfile.NamedTemporaryFile()\n",
    "rpc.init_rpc(\n",
    "    name=\"worker\",\n",
    "    rank=0,\n",
    "    world_size=1,\n",
    "    rpc_backend_options=rpc.TensorPipeRpcBackendOptions(\n",
    "        init_method=\"file://{}\".format(tmpfile.name),\n",
    "        # Specifying _transports and _channels is a workaround and we no longer\n",
    "        # will have to specify _transports and _channels for PyTorch\n",
    "        # versions >= 1.8.1\n",
    "        _transports=[\"ibv\", \"uv\"],\n",
    "        _channels=[\"cuda_ipc\", \"cuda_basic\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "save_model = Pipe(torch.nn.Sequential(*tmp_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(save_model, 'pipe_obj.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa943cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceGANBert(nn.Module):\n",
    "    \n",
    "    def __init__(self, transformer, discriminator):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer\n",
    "        self.transformer.eval() \n",
    "        self.discriminator = discriminator\n",
    "        self.discriminator.eval()\n",
    "        \n",
    "    def forward(self, dataloader, batch_size=64):\n",
    "        # do the forward pass\n",
    "        \n",
    "        device = get_gpu_details()\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            with torch.no_grad():        \n",
    "                model_outputs = self.transformer(b_input_ids, attention_mask=b_input_mask)\n",
    "                hidden_states = model_outputs[-1]\n",
    "                _, _, probs = self.discriminator(hidden_states)\n",
    "                predicted_probs.extend(probs)\n",
    "                \n",
    "        return predicted_probs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d71060a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parallel.data_parallel.DataParallel"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b52c415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ganbert_models.Discriminator"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10a8eabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.tokenization_bert_fast.BertTokenizerFast"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2219f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
